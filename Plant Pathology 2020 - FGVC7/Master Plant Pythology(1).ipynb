{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **MultiLabelClassification Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization,Activation,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.applications import DenseNet121\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/sample_submission.csv\")\n",
    "test = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\n",
    "train = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100\n",
    "train_image = []\n",
    "for i,name in enumerate(train['image_id']):\n",
    "    path = '/kaggle/input/plant-pathology-2020-fgvc7/images/'+name+'.jpg'\n",
    "    img = cv2.imread(path)\n",
    "    image = cv2.resize(img,(img_size,img_size),interpolation = cv2.INTER_AREA)\n",
    "    train_image.append(image)\n",
    "    if i%200==0:\n",
    "        print(i, 'images processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "for i in range(4):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(train_image[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 100\n",
    "test_image = []\n",
    "for i,name in enumerate(test['image_id']):\n",
    "    path = '/kaggle/input/plant-pathology-2020-fgvc7/images/'+name+'.jpg'\n",
    "    img = cv2.imread(path)\n",
    "    image = cv2.resize(img,(img_size,img_size),interpolation = cv2.INTER_AREA)\n",
    "    test_image.append(image)\n",
    "    if i%200==0:\n",
    "        print(i, 'images processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "for i in range(4):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(test_image[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Reshaping and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = np.ndarray(shape=(len(train_image), img_size, img_size, 3),dtype = np.float32)\n",
    "i=0\n",
    "for image in train_image:\n",
    "    #X_Train[i]=img_to_array(image)\n",
    "    X_Train[i]=train_image[i]\n",
    "    i=i+1\n",
    "X_Train=X_Train/255\n",
    "print('Train Shape: {}'.format(X_Train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test = np.ndarray(shape=(len(test_image), img_size, img_size, 3),dtype = np.float32)\n",
    "i=0\n",
    "for image in test_image:\n",
    "    #X_Test[i]=img_to_array(image)\n",
    "    X_Test[i]=test_image[i]\n",
    "    i=i+1\n",
    "    \n",
    "X_Test=X_Test/255\n",
    "print('Test Shape: {}'.format(X_Test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.copy()\n",
    "del y['image_id']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y.values)\n",
    "print(y_train.shape,y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_Train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior =  efn.EfficientNetB7(weights='imagenet', include_top=False, pooling='avg', input_shape=(img_size, img_size, 3))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(prior)\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "    # multi output\n",
    "model.add(tf.keras.layers.Dense(4,activation = 'softmax'))\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
    "checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "# Generates batches of image data with data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
    "                        width_shift_range=0.2, # Range for random horizontal shifts\n",
    "                        height_shift_range=0.2, # Range for random vertical shifts\n",
    "                        zoom_range=0.2, # Range for random zoom\n",
    "                        horizontal_flip=True, # Randomly flip inputs horizontally\n",
    "                        vertical_flip=True) # Randomly flip inputs vertically\n",
    "\n",
    "datagen.fit(X_train)\n",
    "# Fits the model on batches with real-time data augmentation\n",
    "hist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "               steps_per_epoch=X_train.shape[0] // 32,\n",
    "               epochs=80,\n",
    "               verbose=1,\n",
    "               callbacks=[annealer, checkpoint],\n",
    "               validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_Test)\n",
    "all_predict = np.ndarray(shape = (test.shape[0],4),dtype = np.float32)\n",
    "for i in range(0,test.shape[0]):\n",
    "    for j in range(0,4):\n",
    "        if predict[i][j]==max(predict[i]):\n",
    "            all_predict[i][j] = 1\n",
    "        else:\n",
    "            all_predict[i][j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy = [y_test[0] for y_test in all_predict]\n",
    "multiple_diseases = [y_test[1] for y_test in all_predict]\n",
    "rust = [y_test[2] for y_test in all_predict]\n",
    "scab = [y_test[3] for y_test in all_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'image_id':test.image_id,'healthy':healthy,'multiple_diseases':multiple_diseases,'rust':rust,'scab':scab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('efn_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = tf.keras.applications.DenseNet121(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (img_size,img_size,3)\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(prior)\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "    # multi output\n",
    "model.add(tf.keras.layers.Dense(4,activation = 'softmax'))\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
    "checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "# Generates batches of image data with data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
    "                        width_shift_range=0.2, # Range for random horizontal shifts\n",
    "                        height_shift_range=0.2, # Range for random vertical shifts\n",
    "                        zoom_range=0.2, # Range for random zoom\n",
    "                        horizontal_flip=True, # Randomly flip inputs horizontally\n",
    "                        vertical_flip=True) # Randomly flip inputs vertically\n",
    "\n",
    "datagen.fit(X_train)\n",
    "# Fits the model on batches with real-time data augmentation\n",
    "hist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "               steps_per_epoch=X_train.shape[0] // 32,\n",
    "               epochs=80,\n",
    "               verbose=1,\n",
    "               callbacks=[annealer, checkpoint],\n",
    "               validation_data=(X_val, Y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_Test)\n",
    "all_predict = np.ndarray(shape = (test.shape[0],4),dtype = np.float32)\n",
    "for i in range(0,test.shape[0]):\n",
    "    for j in range(0,4):\n",
    "        if predict[i][j]==max(predict[i]):\n",
    "            all_predict[i][j] = 1\n",
    "        else:\n",
    "            all_predict[i][j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy = [y_test[0] for y_test in all_predict]\n",
    "multiple_diseases = [y_test[1] for y_test in all_predict]\n",
    "rust = [y_test[2] for y_test in all_predict]\n",
    "scab = [y_test[3] for y_test in all_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'image_id':test.image_id,'healthy':healthy,'multiple_diseases':multiple_diseases,'rust':rust,'scab':scab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
