{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,SimpleRNN\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('dengue_features_train.csv')\n",
    "test = pd.read_csv('dengue_features_test.csv')\n",
    "labels= pd.read_csv('dengue_labels_train.csv')\n",
    "sub= pd.read_csv('submission_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_cases\n",
       "0            4\n",
       "1            5\n",
       "2            4\n",
       "3            3\n",
       "4            6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cpy = labels.copy()\n",
    "labels_cpy.drop(['city','year','weekofyear'],axis=1,inplace=True)\n",
    "labels_cpy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.198483</td>\n",
       "      <td>0.177617</td>\n",
       "      <td>12.42</td>\n",
       "      <td>297.572857</td>\n",
       "      <td>...</td>\n",
       "      <td>73.365714</td>\n",
       "      <td>12.42</td>\n",
       "      <td>14.012857</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>25.442857</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>29.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-05-07</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>22.82</td>\n",
       "      <td>298.211429</td>\n",
       "      <td>...</td>\n",
       "      <td>77.368571</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.372857</td>\n",
       "      <td>2.371429</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>31.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>1990-05-14</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>34.54</td>\n",
       "      <td>298.781429</td>\n",
       "      <td>...</td>\n",
       "      <td>82.052857</td>\n",
       "      <td>34.54</td>\n",
       "      <td>16.848571</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.485714</td>\n",
       "      <td>32.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>1990-05-21</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>0.245067</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.235886</td>\n",
       "      <td>15.36</td>\n",
       "      <td>298.987143</td>\n",
       "      <td>...</td>\n",
       "      <td>80.337143</td>\n",
       "      <td>15.36</td>\n",
       "      <td>16.672857</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>27.471429</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>1990-05-28</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.247340</td>\n",
       "      <td>7.52</td>\n",
       "      <td>299.518571</td>\n",
       "      <td>...</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>7.52</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>3.014286</td>\n",
       "      <td>28.942857</td>\n",
       "      <td>9.371429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n",
       "1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n",
       "2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n",
       "3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n",
       "4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n",
       "\n",
       "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "0  0.177617                 12.42             297.572857  ...   \n",
       "1  0.155486                 22.82             298.211429  ...   \n",
       "2  0.170843                 34.54             298.781429  ...   \n",
       "3  0.235886                 15.36             298.987143  ...   \n",
       "4  0.247340                  7.52             299.518571  ...   \n",
       "\n",
       "   reanalysis_relative_humidity_percent  reanalysis_sat_precip_amt_mm  \\\n",
       "0                             73.365714                         12.42   \n",
       "1                             77.368571                         22.82   \n",
       "2                             82.052857                         34.54   \n",
       "3                             80.337143                         15.36   \n",
       "4                             80.460000                          7.52   \n",
       "\n",
       "   reanalysis_specific_humidity_g_per_kg  reanalysis_tdtr_k  \\\n",
       "0                              14.012857           2.628571   \n",
       "1                              15.372857           2.371429   \n",
       "2                              16.848571           2.300000   \n",
       "3                              16.672857           2.428571   \n",
       "4                              17.210000           3.014286   \n",
       "\n",
       "   station_avg_temp_c  station_diur_temp_rng_c  station_max_temp_c  \\\n",
       "0           25.442857                 6.900000                29.4   \n",
       "1           26.714286                 6.371429                31.7   \n",
       "2           26.714286                 6.485714                32.2   \n",
       "3           27.471429                 6.771429                33.3   \n",
       "4           28.942857                 9.371429                35.0   \n",
       "\n",
       "   station_min_temp_c  station_precip_mm  total_cases  \n",
       "0                20.0               16.0            4  \n",
       "1                22.2                8.6            5  \n",
       "2                22.8               41.4            4  \n",
       "3                23.3                4.0            3  \n",
       "4                23.9                5.8            6  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = train.join(labels_cpy)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_precip_amt_kg_per_m2</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>18</td>\n",
       "      <td>2008-04-29</td>\n",
       "      <td>-0.0189</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>0.102729</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>78.60</td>\n",
       "      <td>298.492857</td>\n",
       "      <td>...</td>\n",
       "      <td>25.37</td>\n",
       "      <td>78.781429</td>\n",
       "      <td>78.60</td>\n",
       "      <td>15.918571</td>\n",
       "      <td>3.128571</td>\n",
       "      <td>26.528571</td>\n",
       "      <td>7.057143</td>\n",
       "      <td>33.3</td>\n",
       "      <td>21.7</td>\n",
       "      <td>75.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>19</td>\n",
       "      <td>2008-05-06</td>\n",
       "      <td>-0.0180</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>0.082043</td>\n",
       "      <td>0.072314</td>\n",
       "      <td>12.56</td>\n",
       "      <td>298.475714</td>\n",
       "      <td>...</td>\n",
       "      <td>21.83</td>\n",
       "      <td>78.230000</td>\n",
       "      <td>12.56</td>\n",
       "      <td>15.791429</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>26.071429</td>\n",
       "      <td>5.557143</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>34.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>20</td>\n",
       "      <td>2008-05-13</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151083</td>\n",
       "      <td>0.091529</td>\n",
       "      <td>3.66</td>\n",
       "      <td>299.455714</td>\n",
       "      <td>...</td>\n",
       "      <td>4.12</td>\n",
       "      <td>78.270000</td>\n",
       "      <td>3.66</td>\n",
       "      <td>16.674286</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>27.928571</td>\n",
       "      <td>7.785714</td>\n",
       "      <td>32.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>21</td>\n",
       "      <td>2008-05-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019867</td>\n",
       "      <td>0.124329</td>\n",
       "      <td>0.125686</td>\n",
       "      <td>0.00</td>\n",
       "      <td>299.690000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>73.015714</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.775714</td>\n",
       "      <td>4.342857</td>\n",
       "      <td>28.057143</td>\n",
       "      <td>6.271429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>22</td>\n",
       "      <td>2008-05-27</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.039833</td>\n",
       "      <td>0.062267</td>\n",
       "      <td>0.075914</td>\n",
       "      <td>0.76</td>\n",
       "      <td>299.780000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.36</td>\n",
       "      <td>74.084286</td>\n",
       "      <td>0.76</td>\n",
       "      <td>16.137143</td>\n",
       "      <td>3.542857</td>\n",
       "      <td>27.614286</td>\n",
       "      <td>7.085714</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>84.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear week_start_date  ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0   sj  2008          18      2008-04-29  -0.0189 -0.018900  0.102729   \n",
       "1   sj  2008          19      2008-05-06  -0.0180 -0.012400  0.082043   \n",
       "2   sj  2008          20      2008-05-13  -0.0015       NaN  0.151083   \n",
       "3   sj  2008          21      2008-05-20      NaN -0.019867  0.124329   \n",
       "4   sj  2008          22      2008-05-27   0.0568  0.039833  0.062267   \n",
       "\n",
       "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "0  0.091200                 78.60             298.492857  ...   \n",
       "1  0.072314                 12.56             298.475714  ...   \n",
       "2  0.091529                  3.66             299.455714  ...   \n",
       "3  0.125686                  0.00             299.690000  ...   \n",
       "4  0.075914                  0.76             299.780000  ...   \n",
       "\n",
       "   reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
       "0                            25.37                             78.781429   \n",
       "1                            21.83                             78.230000   \n",
       "2                             4.12                             78.270000   \n",
       "3                             2.20                             73.015714   \n",
       "4                             4.36                             74.084286   \n",
       "\n",
       "   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
       "0                         78.60                              15.918571   \n",
       "1                         12.56                              15.791429   \n",
       "2                          3.66                              16.674286   \n",
       "3                          0.00                              15.775714   \n",
       "4                          0.76                              16.137143   \n",
       "\n",
       "   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
       "0           3.128571           26.528571                 7.057143   \n",
       "1           2.571429           26.071429                 5.557143   \n",
       "2           4.428571           27.928571                 7.785714   \n",
       "3           4.342857           28.057143                 6.271429   \n",
       "4           3.542857           27.614286                 7.085714   \n",
       "\n",
       "   station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
       "0                33.3                21.7               75.2  \n",
       "1                30.0                22.2               34.3  \n",
       "2                32.8                22.8                3.0  \n",
       "3                33.3                24.4                0.3  \n",
       "4                33.3                23.3               84.1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data():\n",
    "    #get train data\n",
    "    train = pd.read_csv('dengue_features_train.csv')\n",
    "    \n",
    "    #get test data\n",
    "    test = pd.read_csv('dengue_features_test.csv')\n",
    "    \n",
    "    return train , test\n",
    "\n",
    "\n",
    "def get_combined_data():\n",
    "  #reading train data\n",
    "  train , test = get_data()\n",
    "\n",
    "  target = final_df.total_cases\n",
    "  final_df.drop(['total_cases'],axis = 1 , inplace = True)\n",
    "\n",
    "  combined = train.append(test)\n",
    "  combined.reset_index(inplace=True)\n",
    "  combined.drop(['index'], inplace=True, axis=1)\n",
    "  return combined, target\n",
    "\n",
    "#Load train and test data into pandas DataFrames\n",
    "train_data, test_data = get_data()\n",
    "\n",
    "#Combine train and test data to process them together\n",
    "combined, target = get_combined_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_precip_amt_kg_per_m2</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>22</td>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>0.301471</td>\n",
       "      <td>0.380029</td>\n",
       "      <td>0.280629</td>\n",
       "      <td>0.383186</td>\n",
       "      <td>41.12</td>\n",
       "      <td>297.774286</td>\n",
       "      <td>...</td>\n",
       "      <td>67.60</td>\n",
       "      <td>89.990000</td>\n",
       "      <td>41.12</td>\n",
       "      <td>17.185714</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>27.40</td>\n",
       "      <td>9.050</td>\n",
       "      <td>32.6</td>\n",
       "      <td>21.8</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>23</td>\n",
       "      <td>2013-06-04</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.296343</td>\n",
       "      <td>0.285371</td>\n",
       "      <td>0.350357</td>\n",
       "      <td>71.52</td>\n",
       "      <td>297.167143</td>\n",
       "      <td>...</td>\n",
       "      <td>45.70</td>\n",
       "      <td>93.891429</td>\n",
       "      <td>71.52</td>\n",
       "      <td>17.448571</td>\n",
       "      <td>9.657143</td>\n",
       "      <td>27.52</td>\n",
       "      <td>10.720</td>\n",
       "      <td>33.8</td>\n",
       "      <td>21.4</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>24</td>\n",
       "      <td>2013-06-11</td>\n",
       "      <td>0.238729</td>\n",
       "      <td>0.251029</td>\n",
       "      <td>0.252586</td>\n",
       "      <td>0.249771</td>\n",
       "      <td>78.96</td>\n",
       "      <td>295.831429</td>\n",
       "      <td>...</td>\n",
       "      <td>45.22</td>\n",
       "      <td>94.967143</td>\n",
       "      <td>78.96</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>7.385714</td>\n",
       "      <td>27.20</td>\n",
       "      <td>10.075</td>\n",
       "      <td>32.6</td>\n",
       "      <td>21.6</td>\n",
       "      <td>93.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>25</td>\n",
       "      <td>2013-06-18</td>\n",
       "      <td>0.310429</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>0.406614</td>\n",
       "      <td>0.403943</td>\n",
       "      <td>39.54</td>\n",
       "      <td>295.778571</td>\n",
       "      <td>...</td>\n",
       "      <td>4.70</td>\n",
       "      <td>89.057143</td>\n",
       "      <td>39.54</td>\n",
       "      <td>15.137143</td>\n",
       "      <td>8.228571</td>\n",
       "      <td>26.70</td>\n",
       "      <td>8.480</td>\n",
       "      <td>32.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>34.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>iq</td>\n",
       "      <td>2013</td>\n",
       "      <td>26</td>\n",
       "      <td>2013-06-25</td>\n",
       "      <td>0.339467</td>\n",
       "      <td>0.240071</td>\n",
       "      <td>0.356943</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>51.80</td>\n",
       "      <td>297.372857</td>\n",
       "      <td>...</td>\n",
       "      <td>27.80</td>\n",
       "      <td>87.030000</td>\n",
       "      <td>51.80</td>\n",
       "      <td>16.148571</td>\n",
       "      <td>11.542857</td>\n",
       "      <td>27.35</td>\n",
       "      <td>9.675</td>\n",
       "      <td>32.6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "1867   iq  2013          22      2013-05-28  0.301471  0.380029  0.280629   \n",
       "1868   iq  2013          23      2013-06-04  0.247600  0.296343  0.285371   \n",
       "1869   iq  2013          24      2013-06-11  0.238729  0.251029  0.252586   \n",
       "1870   iq  2013          25      2013-06-18  0.310429  0.302700  0.406614   \n",
       "1871   iq  2013          26      2013-06-25  0.339467  0.240071  0.356943   \n",
       "\n",
       "       ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "1867  0.383186                 41.12             297.774286  ...   \n",
       "1868  0.350357                 71.52             297.167143  ...   \n",
       "1869  0.249771                 78.96             295.831429  ...   \n",
       "1870  0.403943                 39.54             295.778571  ...   \n",
       "1871  0.273600                 51.80             297.372857  ...   \n",
       "\n",
       "      reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
       "1867                            67.60                             89.990000   \n",
       "1868                            45.70                             93.891429   \n",
       "1869                            45.22                             94.967143   \n",
       "1870                             4.70                             89.057143   \n",
       "1871                            27.80                             87.030000   \n",
       "\n",
       "      reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
       "1867                         41.12                              17.185714   \n",
       "1868                         71.52                              17.448571   \n",
       "1869                         78.96                              16.410000   \n",
       "1870                         39.54                              15.137143   \n",
       "1871                         51.80                              16.148571   \n",
       "\n",
       "      reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
       "1867          10.100000               27.40                    9.050   \n",
       "1868           9.657143               27.52                   10.720   \n",
       "1869           7.385714               27.20                   10.075   \n",
       "1870           8.228571               26.70                    8.480   \n",
       "1871          11.542857               27.35                    9.675   \n",
       "\n",
       "      station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
       "1867                32.6                21.8               33.0  \n",
       "1868                33.8                21.4               68.0  \n",
       "1869                32.6                21.6               93.2  \n",
       "1870                32.2                21.8               34.1  \n",
       "1871                32.6                22.0               14.9  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    5\n",
       "2    4\n",
       "3    3\n",
       "4    6\n",
       "Name: total_cases, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_with_no_nans(df,col_type):\n",
    "    '''\n",
    "    Arguments :\n",
    "    df : The dataframe to process\n",
    "    col_type : \n",
    "          num : to only get numerical columns with no nans\n",
    "          no_num : to only get nun-numerical columns with no nans\n",
    "          all : to get any columns with no nans    \n",
    "    '''\n",
    "    if (col_type == 'num'):\n",
    "        predictors = df.select_dtypes(exclude=['object'])\n",
    "    elif (col_type == 'no_num'):\n",
    "        predictors = df.select_dtypes(include=['object'])\n",
    "    elif (col_type == 'all'):\n",
    "        predictors = df\n",
    "    else :\n",
    "        print('Error : choose a type (num, no_num, all)')\n",
    "        return 0\n",
    "    cols_with_no_nans = []\n",
    "    for col in predictors.columns:\n",
    "        if not df[col].isnull().any():\n",
    "            cols_with_no_nans.append(col)\n",
    "    return cols_with_no_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = get_cols_with_no_nans(combined , 'num')\n",
    "cat_cols = get_cols_with_no_nans(combined , 'no_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numerical columns with no nan values : 2\n",
      "Number of nun-numerical columns with no nan values : 2\n"
     ]
    }
   ],
   "source": [
    "print ('Number of numerical columns with no nan values :',len(num_cols))\n",
    "print ('Number of nun-numerical columns with no nan values :',len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined = combined[num_cols + cat_cols]\n",
    "combined.hist(figsize = (12,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data[num_cols + cat_cols]\n",
    "train_data['Target'] = target\n",
    "\n",
    "C_mat = train_data.corr()\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "\n",
    "sb.heatmap(C_mat, vmax = .8, square = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 4 columns before encoding categorical features\n",
      "There are 1209 columns after encoding categorical features\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "            #drop the encoded column\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\n",
    "combined = oneHotEncode(combined, cat_cols)\n",
    "print('There are {} columns after encoding categorical features'.format(combined.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1872, 1209)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_combined():\n",
    "    global combined\n",
    "    train = combined[:1456]\n",
    "    test = combined[1456:]\n",
    "\n",
    "    return train , test \n",
    "  \n",
    "train, test = split_combined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9       , 0.32692308, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.9       , 0.34615385, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.9       , 0.36538462, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [1.15      , 0.44230769, 1.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.15      , 0.46153846, 1.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [1.15      , 0.48076923, 1.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit_transform(train)\n",
    "scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 1024)              1239040   \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,641,393\n",
      "Trainable params: 2,641,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(1024,input_dim = train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(1024,activation='relu'))\n",
    "NN_model.add(Dense(256,activation='relu'))\n",
    "NN_model.add(Dense(256,activation='relu'))\n",
    "NN_model.add(Dense(64,activation='relu'))\n",
    "NN_model.add(Dense(64,activation='relu'))\n",
    "NN_model.add(Dense(32,activation='relu'))\n",
    "NN_model.add(Dense(32,activation='relu'))\n",
    "NN_model.add(Dense(16,activation='relu'))\n",
    "NN_model.add(Dense(16,activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 4.3464 - mean_absolute_error: 4.3464 - val_loss: 7.8775 - val_mean_absolute_error: 7.8775\n",
      "Epoch 2/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.1458 - mean_absolute_error: 4.1458 - val_loss: 7.9063 - val_mean_absolute_error: 7.9063\n",
      "Epoch 3/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.5117 - mean_absolute_error: 4.5117 - val_loss: 7.9559 - val_mean_absolute_error: 7.9559\n",
      "Epoch 4/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 9.2129 - mean_absolute_error: 9.2129 - val_loss: 7.6202 - val_mean_absolute_error: 7.6202\n",
      "Epoch 5/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.1974 - mean_absolute_error: 5.1974 - val_loss: 7.8511 - val_mean_absolute_error: 7.8511\n",
      "Epoch 6/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 4.0247 - mean_absolute_error: 4.0247 - val_loss: 7.7152 - val_mean_absolute_error: 7.7152\n",
      "Epoch 7/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.0626 - mean_absolute_error: 5.0626 - val_loss: 7.9261 - val_mean_absolute_error: 7.9261\n",
      "Epoch 8/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 8.2205 - mean_absolute_error: 8.2205 - val_loss: 7.0755 - val_mean_absolute_error: 7.0755\n",
      "Epoch 9/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 5.9682 - mean_absolute_error: 5.9682 - val_loss: 7.7659 - val_mean_absolute_error: 7.7659\n",
      "Epoch 10/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.7583 - mean_absolute_error: 4.7583 - val_loss: 7.7950 - val_mean_absolute_error: 7.7950\n",
      "Epoch 11/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 5.9079 - mean_absolute_error: 5.9079 - val_loss: 7.9172 - val_mean_absolute_error: 7.9172\n",
      "Epoch 12/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 4.7522 - mean_absolute_error: 4.7522 - val_loss: 7.8572 - val_mean_absolute_error: 7.8572\n",
      "Epoch 13/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 4.7482 - mean_absolute_error: 4.7482 - val_loss: 7.9357 - val_mean_absolute_error: 7.9357\n",
      "Epoch 14/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.3659 - mean_absolute_error: 4.3659 - val_loss: 7.8310 - val_mean_absolute_error: 7.8310\n",
      "Epoch 15/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 8.4107 - mean_absolute_error: 8.4107 - val_loss: 7.8823 - val_mean_absolute_error: 7.8823\n",
      "Epoch 16/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 5.7791 - mean_absolute_error: 5.7791 - val_loss: 7.7893 - val_mean_absolute_error: 7.7893\n",
      "Epoch 17/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.4755 - mean_absolute_error: 4.4755 - val_loss: 7.7731 - val_mean_absolute_error: 7.7731\n",
      "Epoch 18/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.6684 - mean_absolute_error: 3.6684 - val_loss: 7.7878 - val_mean_absolute_error: 7.7877\n",
      "Epoch 19/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.0547 - mean_absolute_error: 3.0547 - val_loss: 7.9439 - val_mean_absolute_error: 7.9439\n",
      "Epoch 20/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.9085 - mean_absolute_error: 4.9085 - val_loss: 7.7174 - val_mean_absolute_error: 7.7174\n",
      "Epoch 21/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5242 - mean_absolute_error: 3.5242 - val_loss: 7.8970 - val_mean_absolute_error: 7.8970\n",
      "Epoch 22/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.6383 - mean_absolute_error: 3.6383 - val_loss: 7.8611 - val_mean_absolute_error: 7.8611\n",
      "Epoch 23/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.2089 - mean_absolute_error: 5.2089 - val_loss: 7.9850 - val_mean_absolute_error: 7.9850\n",
      "Epoch 24/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.9210 - mean_absolute_error: 5.9210 - val_loss: 7.6598 - val_mean_absolute_error: 7.6598\n",
      "Epoch 25/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 5.6242 - mean_absolute_error: 5.6242 - val_loss: 7.7522 - val_mean_absolute_error: 7.7522\n",
      "Epoch 26/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 5.1673 - mean_absolute_error: 5.1673 - val_loss: 7.9216 - val_mean_absolute_error: 7.9216\n",
      "Epoch 27/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 5.2644 - mean_absolute_error: 5.2644 - val_loss: 7.8651 - val_mean_absolute_error: 7.8651\n",
      "Epoch 28/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 5.5428 - mean_absolute_error: 5.5428 - val_loss: 7.7862 - val_mean_absolute_error: 7.7862\n",
      "Epoch 29/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.8757 - mean_absolute_error: 4.8757 - val_loss: 7.8448 - val_mean_absolute_error: 7.8448\n",
      "Epoch 30/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.9013 - mean_absolute_error: 4.9013 - val_loss: 7.7944 - val_mean_absolute_error: 7.7944\n",
      "Epoch 31/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 5.0064 - mean_absolute_error: 5.0064 - val_loss: 7.6102 - val_mean_absolute_error: 7.6102\n",
      "Epoch 32/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.7584 - mean_absolute_error: 4.7584 - val_loss: 7.8444 - val_mean_absolute_error: 7.8444\n",
      "Epoch 33/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.3497 - mean_absolute_error: 4.3497 - val_loss: 7.7458 - val_mean_absolute_error: 7.7458\n",
      "Epoch 34/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.3692 - mean_absolute_error: 4.3692 - val_loss: 7.6742 - val_mean_absolute_error: 7.6742\n",
      "Epoch 35/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 8.5957 - mean_absolute_error: 8.5957 - val_loss: 7.0345 - val_mean_absolute_error: 7.0345\n",
      "Epoch 36/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 7.4134 - mean_absolute_error: 7.4134 - val_loss: 7.4909 - val_mean_absolute_error: 7.4909\n",
      "Epoch 37/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 5.7796 - mean_absolute_error: 5.7796 - val_loss: 7.9212 - val_mean_absolute_error: 7.9212\n",
      "Epoch 38/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.6236 - mean_absolute_error: 4.6236 - val_loss: 7.6721 - val_mean_absolute_error: 7.6721\n",
      "Epoch 39/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 6.8872 - mean_absolute_error: 6.8872 - val_loss: 7.8205 - val_mean_absolute_error: 7.8205\n",
      "Epoch 40/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.8788 - mean_absolute_error: 3.8788 - val_loss: 7.5780 - val_mean_absolute_error: 7.5780\n",
      "Epoch 41/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.1756 - mean_absolute_error: 4.1756 - val_loss: 7.7587 - val_mean_absolute_error: 7.7587\n",
      "Epoch 42/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.8675 - mean_absolute_error: 3.8675 - val_loss: 7.9203 - val_mean_absolute_error: 7.9203\n",
      "Epoch 43/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.7062 - mean_absolute_error: 4.7062 - val_loss: 7.8793 - val_mean_absolute_error: 7.8793\n",
      "Epoch 44/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.7334 - mean_absolute_error: 3.7334 - val_loss: 7.8981 - val_mean_absolute_error: 7.8981\n",
      "Epoch 45/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.1440 - mean_absolute_error: 4.1439 - val_loss: 7.7647 - val_mean_absolute_error: 7.7647\n",
      "Epoch 46/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.7494 - mean_absolute_error: 2.7494 - val_loss: 7.7670 - val_mean_absolute_error: 7.7670\n",
      "Epoch 47/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.3013 - mean_absolute_error: 3.3013 - val_loss: 7.8170 - val_mean_absolute_error: 7.8170\n",
      "Epoch 48/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.0409 - mean_absolute_error: 3.0409 - val_loss: 7.7023 - val_mean_absolute_error: 7.7023\n",
      "Epoch 49/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 4.1164 - mean_absolute_error: 4.1164 - val_loss: 7.9903 - val_mean_absolute_error: 7.9903\n",
      "Epoch 50/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.7003 - mean_absolute_error: 5.7003 - val_loss: 7.6699 - val_mean_absolute_error: 7.6699\n",
      "Epoch 51/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.2207 - mean_absolute_error: 4.2207 - val_loss: 7.6755 - val_mean_absolute_error: 7.6755\n",
      "Epoch 52/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.2551 - mean_absolute_error: 3.2551 - val_loss: 7.7649 - val_mean_absolute_error: 7.7649\n",
      "Epoch 53/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 4.0818 - mean_absolute_error: 4.0818 - val_loss: 7.7179 - val_mean_absolute_error: 7.7179\n",
      "Epoch 54/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.4303 - mean_absolute_error: 3.4303 - val_loss: 7.7197 - val_mean_absolute_error: 7.7197\n",
      "Epoch 55/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.3545 - mean_absolute_error: 3.3545 - val_loss: 8.1549 - val_mean_absolute_error: 8.1549\n",
      "Epoch 56/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.6516 - mean_absolute_error: 4.6516 - val_loss: 7.5773 - val_mean_absolute_error: 7.5773\n",
      "Epoch 57/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.2442 - mean_absolute_error: 4.2442 - val_loss: 7.9666 - val_mean_absolute_error: 7.9666\n",
      "Epoch 58/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 5.0849 - mean_absolute_error: 5.0849 - val_loss: 7.4588 - val_mean_absolute_error: 7.4588\n",
      "Epoch 59/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 6.5377 - mean_absolute_error: 6.5377 - val_loss: 7.9426 - val_mean_absolute_error: 7.9426\n",
      "Epoch 60/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 4.6441 - mean_absolute_error: 4.6441 - val_loss: 7.4874 - val_mean_absolute_error: 7.4874\n",
      "Epoch 61/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 5.5061 - mean_absolute_error: 5.5061 - val_loss: 7.8087 - val_mean_absolute_error: 7.8087\n",
      "Epoch 62/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 4.6645 - mean_absolute_error: 4.6645 - val_loss: 7.7189 - val_mean_absolute_error: 7.7189\n",
      "Epoch 63/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 6.0617 - mean_absolute_error: 6.0617 - val_loss: 7.6480 - val_mean_absolute_error: 7.6480\n",
      "Epoch 64/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.2953 - mean_absolute_error: 3.2953 - val_loss: 7.6050 - val_mean_absolute_error: 7.6050\n",
      "Epoch 65/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 4.1592 - mean_absolute_error: 4.1592 - val_loss: 7.6514 - val_mean_absolute_error: 7.6514\n",
      "Epoch 66/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.6563 - mean_absolute_error: 4.6563 - val_loss: 7.7384 - val_mean_absolute_error: 7.7384\n",
      "Epoch 67/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 5.5360 - mean_absolute_error: 5.5360 - val_loss: 7.6253 - val_mean_absolute_error: 7.6253\n",
      "Epoch 68/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 9.8195 - mean_absolute_error: 9.8195 - val_loss: 7.5439 - val_mean_absolute_error: 7.5439\n",
      "Epoch 69/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.5640 - mean_absolute_error: 3.5640 - val_loss: 7.6144 - val_mean_absolute_error: 7.6144\n",
      "Epoch 70/800\n",
      "1164/1164 [==============================] - 0s 132us/sample - loss: 3.5229 - mean_absolute_error: 3.5229 - val_loss: 7.8082 - val_mean_absolute_error: 7.8082\n",
      "Epoch 71/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 5.4918 - mean_absolute_error: 5.4918 - val_loss: 7.5903 - val_mean_absolute_error: 7.5903\n",
      "Epoch 72/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.9401 - mean_absolute_error: 2.9401 - val_loss: 7.9309 - val_mean_absolute_error: 7.9309\n",
      "Epoch 73/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.2649 - mean_absolute_error: 3.2649 - val_loss: 7.9267 - val_mean_absolute_error: 7.9267\n",
      "Epoch 74/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.8832 - mean_absolute_error: 4.8832 - val_loss: 7.9337 - val_mean_absolute_error: 7.9337\n",
      "Epoch 75/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 6.2957 - mean_absolute_error: 6.2957 - val_loss: 7.8528 - val_mean_absolute_error: 7.8528\n",
      "Epoch 76/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 6.5231 - mean_absolute_error: 6.5231 - val_loss: 7.6089 - val_mean_absolute_error: 7.6089\n",
      "Epoch 77/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.7610 - mean_absolute_error: 3.7610 - val_loss: 7.2814 - val_mean_absolute_error: 7.2814\n",
      "Epoch 78/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 5.1450 - mean_absolute_error: 5.1450 - val_loss: 7.7216 - val_mean_absolute_error: 7.7216\n",
      "Epoch 79/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.0573 - mean_absolute_error: 3.0573 - val_loss: 7.9080 - val_mean_absolute_error: 7.9080\n",
      "Epoch 80/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.4802 - mean_absolute_error: 4.4802 - val_loss: 8.0150 - val_mean_absolute_error: 8.0150\n",
      "Epoch 81/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.9411 - mean_absolute_error: 3.9411 - val_loss: 7.8878 - val_mean_absolute_error: 7.8878\n",
      "Epoch 82/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 7.1286 - mean_absolute_error: 7.1286 - val_loss: 7.6585 - val_mean_absolute_error: 7.6585\n",
      "Epoch 83/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.9814 - mean_absolute_error: 5.9814 - val_loss: 7.4752 - val_mean_absolute_error: 7.4752\n",
      "Epoch 84/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 8.9739 - mean_absolute_error: 8.9739 - val_loss: 7.5348 - val_mean_absolute_error: 7.5348\n",
      "Epoch 85/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 7.0054 - mean_absolute_error: 7.0054 - val_loss: 7.9239 - val_mean_absolute_error: 7.9239\n",
      "Epoch 86/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 9.1911 - mean_absolute_error: 9.1911 - val_loss: 7.6971 - val_mean_absolute_error: 7.6971\n",
      "Epoch 87/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.6841 - mean_absolute_error: 5.6841 - val_loss: 7.8050 - val_mean_absolute_error: 7.8050\n",
      "Epoch 88/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 8.9723 - mean_absolute_error: 8.9723 - val_loss: 7.4321 - val_mean_absolute_error: 7.4321\n",
      "Epoch 89/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 7.1318 - mean_absolute_error: 7.1318 - val_loss: 7.4247 - val_mean_absolute_error: 7.4247\n",
      "Epoch 90/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0656 - mean_absolute_error: 4.0656 - val_loss: 7.2732 - val_mean_absolute_error: 7.2732\n",
      "Epoch 91/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.7605 - mean_absolute_error: 4.7605 - val_loss: 7.8248 - val_mean_absolute_error: 7.8248\n",
      "Epoch 92/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 12.5865 - mean_absolute_error: 12.5865 - val_loss: 7.1554 - val_mean_absolute_error: 7.1554\n",
      "Epoch 93/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 7.3801 - mean_absolute_error: 7.3801 - val_loss: 7.3755 - val_mean_absolute_error: 7.3755\n",
      "Epoch 94/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.0255 - mean_absolute_error: 5.0255 - val_loss: 7.8851 - val_mean_absolute_error: 7.8851\n",
      "Epoch 95/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.6751 - mean_absolute_error: 5.6751 - val_loss: 7.6065 - val_mean_absolute_error: 7.6065\n",
      "Epoch 96/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.7920 - mean_absolute_error: 2.7920 - val_loss: 7.5772 - val_mean_absolute_error: 7.5772\n",
      "Epoch 97/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.1874 - mean_absolute_error: 5.1874 - val_loss: 7.6807 - val_mean_absolute_error: 7.6807\n",
      "Epoch 98/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 3.4116 - mean_absolute_error: 3.4116 - val_loss: 7.6256 - val_mean_absolute_error: 7.6256\n",
      "Epoch 99/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.0793 - mean_absolute_error: 5.0793 - val_loss: 7.6177 - val_mean_absolute_error: 7.6177\n",
      "Epoch 100/800\n",
      "1164/1164 [==============================] - 0s 131us/sample - loss: 3.2779 - mean_absolute_error: 3.2779 - val_loss: 7.6180 - val_mean_absolute_error: 7.6180\n",
      "Epoch 101/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 5.1485 - mean_absolute_error: 5.1485 - val_loss: 7.8335 - val_mean_absolute_error: 7.8335\n",
      "Epoch 102/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 5.3399 - mean_absolute_error: 5.3399 - val_loss: 7.2762 - val_mean_absolute_error: 7.2762\n",
      "Epoch 103/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 4.3504 - mean_absolute_error: 4.3504 - val_loss: 7.5497 - val_mean_absolute_error: 7.5497\n",
      "Epoch 104/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.7671 - mean_absolute_error: 4.7671 - val_loss: 7.6103 - val_mean_absolute_error: 7.6103\n",
      "Epoch 105/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 6.9480 - mean_absolute_error: 6.9480 - val_loss: 7.3215 - val_mean_absolute_error: 7.3215\n",
      "Epoch 106/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 4.1818 - mean_absolute_error: 4.1818 - val_loss: 7.6641 - val_mean_absolute_error: 7.6641\n",
      "Epoch 107/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 6.2331 - mean_absolute_error: 6.2331 - val_loss: 7.2170 - val_mean_absolute_error: 7.2170\n",
      "Epoch 108/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 6.0722 - mean_absolute_error: 6.0722 - val_loss: 7.6051 - val_mean_absolute_error: 7.6051\n",
      "Epoch 109/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.3953 - mean_absolute_error: 4.3953 - val_loss: 7.8698 - val_mean_absolute_error: 7.8698\n",
      "Epoch 110/800\n",
      "1164/1164 [==============================] - 0s 155us/sample - loss: 4.6072 - mean_absolute_error: 4.6072 - val_loss: 7.6352 - val_mean_absolute_error: 7.6352\n",
      "Epoch 111/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.1469 - mean_absolute_error: 4.1469 - val_loss: 8.0456 - val_mean_absolute_error: 8.0456\n",
      "Epoch 112/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.1562 - mean_absolute_error: 3.1562 - val_loss: 7.7673 - val_mean_absolute_error: 7.7673\n",
      "Epoch 113/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.8088 - mean_absolute_error: 3.8088 - val_loss: 7.8175 - val_mean_absolute_error: 7.8175\n",
      "Epoch 114/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.0056 - mean_absolute_error: 4.0056 - val_loss: 7.3940 - val_mean_absolute_error: 7.3940\n",
      "Epoch 115/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 4.3730 - mean_absolute_error: 4.3730 - val_loss: 7.6662 - val_mean_absolute_error: 7.6662\n",
      "Epoch 116/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.1411 - mean_absolute_error: 5.1411 - val_loss: 7.7723 - val_mean_absolute_error: 7.7723\n",
      "Epoch 117/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.9260 - mean_absolute_error: 2.9260 - val_loss: 7.7270 - val_mean_absolute_error: 7.7270\n",
      "Epoch 118/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.4556 - mean_absolute_error: 3.4556 - val_loss: 8.0317 - val_mean_absolute_error: 8.0317\n",
      "Epoch 119/800\n",
      "1164/1164 [==============================] - 0s 133us/sample - loss: 5.3405 - mean_absolute_error: 5.3405 - val_loss: 7.8637 - val_mean_absolute_error: 7.8637\n",
      "Epoch 120/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.7197 - mean_absolute_error: 4.7197 - val_loss: 7.7921 - val_mean_absolute_error: 7.7921\n",
      "Epoch 121/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.1411 - mean_absolute_error: 4.1411 - val_loss: 7.3905 - val_mean_absolute_error: 7.3905\n",
      "Epoch 122/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 8.7290 - mean_absolute_error: 8.7290 - val_loss: 7.8187 - val_mean_absolute_error: 7.8187\n",
      "Epoch 123/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 5.7403 - mean_absolute_error: 5.7403 - val_loss: 7.4302 - val_mean_absolute_error: 7.4302\n",
      "Epoch 124/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 4.4057 - mean_absolute_error: 4.4057 - val_loss: 7.5924 - val_mean_absolute_error: 7.5924\n",
      "Epoch 125/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.2635 - mean_absolute_error: 3.2635 - val_loss: 7.5956 - val_mean_absolute_error: 7.5956\n",
      "Epoch 126/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.8297 - mean_absolute_error: 3.8297 - val_loss: 7.7446 - val_mean_absolute_error: 7.7446\n",
      "Epoch 127/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 4.2659 - mean_absolute_error: 4.2659 - val_loss: 7.5521 - val_mean_absolute_error: 7.5521\n",
      "Epoch 128/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.3492 - mean_absolute_error: 4.3492 - val_loss: 7.6296 - val_mean_absolute_error: 7.6296\n",
      "Epoch 129/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.1921 - mean_absolute_error: 3.1921 - val_loss: 7.6769 - val_mean_absolute_error: 7.6769\n",
      "Epoch 130/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.4425 - mean_absolute_error: 2.4425 - val_loss: 7.6515 - val_mean_absolute_error: 7.6515\n",
      "Epoch 131/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.5682 - mean_absolute_error: 5.5682 - val_loss: 7.7487 - val_mean_absolute_error: 7.7487\n",
      "Epoch 132/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 5.6558 - mean_absolute_error: 5.6558 - val_loss: 7.8171 - val_mean_absolute_error: 7.8171\n",
      "Epoch 133/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 6.0625 - mean_absolute_error: 6.0625 - val_loss: 7.0008 - val_mean_absolute_error: 7.0008\n",
      "Epoch 134/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 3.7233 - mean_absolute_error: 3.7233 - val_loss: 7.5569 - val_mean_absolute_error: 7.5569\n",
      "Epoch 135/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.9604 - mean_absolute_error: 2.9604 - val_loss: 7.6699 - val_mean_absolute_error: 7.6699\n",
      "Epoch 136/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 5.0956 - mean_absolute_error: 5.0956 - val_loss: 7.6977 - val_mean_absolute_error: 7.6977\n",
      "Epoch 137/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.2947 - mean_absolute_error: 5.2947 - val_loss: 7.7881 - val_mean_absolute_error: 7.7881\n",
      "Epoch 138/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.4190 - mean_absolute_error: 4.4190 - val_loss: 7.5098 - val_mean_absolute_error: 7.5098\n",
      "Epoch 139/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 7.4646 - mean_absolute_error: 7.4646 - val_loss: 8.0227 - val_mean_absolute_error: 8.0227\n",
      "Epoch 140/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.2564 - mean_absolute_error: 5.2564 - val_loss: 7.3574 - val_mean_absolute_error: 7.3574\n",
      "Epoch 141/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.0432 - mean_absolute_error: 3.0432 - val_loss: 7.2743 - val_mean_absolute_error: 7.2743\n",
      "Epoch 142/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 6.0896 - mean_absolute_error: 6.0896 - val_loss: 7.5310 - val_mean_absolute_error: 7.5310\n",
      "Epoch 143/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.6514 - mean_absolute_error: 4.6514 - val_loss: 7.3846 - val_mean_absolute_error: 7.3846\n",
      "Epoch 144/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.4552 - mean_absolute_error: 4.4552 - val_loss: 7.5085 - val_mean_absolute_error: 7.5085\n",
      "Epoch 145/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.9804 - mean_absolute_error: 3.9804 - val_loss: 7.3860 - val_mean_absolute_error: 7.3860\n",
      "Epoch 146/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.7059 - mean_absolute_error: 2.7059 - val_loss: 7.4608 - val_mean_absolute_error: 7.4608\n",
      "Epoch 147/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.5171 - mean_absolute_error: 4.5171 - val_loss: 7.4490 - val_mean_absolute_error: 7.4490\n",
      "Epoch 148/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.4324 - mean_absolute_error: 3.4324 - val_loss: 7.7334 - val_mean_absolute_error: 7.7334\n",
      "Epoch 149/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 5.0723 - mean_absolute_error: 5.0723 - val_loss: 7.4730 - val_mean_absolute_error: 7.4730\n",
      "Epoch 150/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 3.6948 - mean_absolute_error: 3.6948 - val_loss: 7.7775 - val_mean_absolute_error: 7.7775\n",
      "Epoch 151/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.6043 - mean_absolute_error: 4.6043 - val_loss: 7.8760 - val_mean_absolute_error: 7.8760\n",
      "Epoch 152/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.1326 - mean_absolute_error: 4.1326 - val_loss: 7.2837 - val_mean_absolute_error: 7.2837\n",
      "Epoch 153/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 3.2909 - mean_absolute_error: 3.2909 - val_loss: 7.5009 - val_mean_absolute_error: 7.5009\n",
      "Epoch 154/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.3526 - mean_absolute_error: 3.3526 - val_loss: 7.6549 - val_mean_absolute_error: 7.6549\n",
      "Epoch 155/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 6.8480 - mean_absolute_error: 6.8480 - val_loss: 7.8180 - val_mean_absolute_error: 7.8180\n",
      "Epoch 156/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.5633 - mean_absolute_error: 5.5633 - val_loss: 7.6026 - val_mean_absolute_error: 7.6026\n",
      "Epoch 157/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.0219 - mean_absolute_error: 4.0219 - val_loss: 7.5010 - val_mean_absolute_error: 7.5010\n",
      "Epoch 158/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.2467 - mean_absolute_error: 3.2467 - val_loss: 7.5974 - val_mean_absolute_error: 7.5974\n",
      "Epoch 159/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.4244 - mean_absolute_error: 2.4244 - val_loss: 7.3221 - val_mean_absolute_error: 7.3221\n",
      "Epoch 160/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5848 - mean_absolute_error: 3.5848 - val_loss: 7.8301 - val_mean_absolute_error: 7.8301\n",
      "Epoch 161/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.1240 - mean_absolute_error: 3.1240 - val_loss: 7.5763 - val_mean_absolute_error: 7.5763\n",
      "Epoch 162/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.1000 - mean_absolute_error: 3.1000 - val_loss: 7.5079 - val_mean_absolute_error: 7.5079\n",
      "Epoch 163/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.5931 - mean_absolute_error: 4.5931 - val_loss: 7.4702 - val_mean_absolute_error: 7.4702\n",
      "Epoch 164/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.2893 - mean_absolute_error: 2.2893 - val_loss: 7.6289 - val_mean_absolute_error: 7.6289\n",
      "Epoch 165/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 6.1293 - mean_absolute_error: 6.1293 - val_loss: 7.2530 - val_mean_absolute_error: 7.2530\n",
      "Epoch 166/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 5.2251 - mean_absolute_error: 5.2251 - val_loss: 7.7022 - val_mean_absolute_error: 7.7022\n",
      "Epoch 167/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.7124 - mean_absolute_error: 3.7124 - val_loss: 7.4472 - val_mean_absolute_error: 7.4472\n",
      "Epoch 168/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 11.3171 - mean_absolute_error: 11.3171 - val_loss: 7.0436 - val_mean_absolute_error: 7.0436\n",
      "Epoch 169/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 6.3020 - mean_absolute_error: 6.3020 - val_loss: 8.0534 - val_mean_absolute_error: 8.0534\n",
      "Epoch 170/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.6611 - mean_absolute_error: 3.6611 - val_loss: 7.4666 - val_mean_absolute_error: 7.4666\n",
      "Epoch 171/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.3789 - mean_absolute_error: 3.3789 - val_loss: 7.0843 - val_mean_absolute_error: 7.0843\n",
      "Epoch 172/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 7.3728 - mean_absolute_error: 7.3728 - val_loss: 7.3915 - val_mean_absolute_error: 7.3915\n",
      "Epoch 173/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.3852 - mean_absolute_error: 5.3852 - val_loss: 7.3818 - val_mean_absolute_error: 7.3818\n",
      "Epoch 174/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 3.5086 - mean_absolute_error: 3.5086 - val_loss: 7.6457 - val_mean_absolute_error: 7.6457\n",
      "Epoch 175/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.9609 - mean_absolute_error: 3.9609 - val_loss: 7.5059 - val_mean_absolute_error: 7.5059\n",
      "Epoch 176/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.3114 - mean_absolute_error: 3.3114 - val_loss: 7.5543 - val_mean_absolute_error: 7.5543\n",
      "Epoch 177/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 6.1608 - mean_absolute_error: 6.1608 - val_loss: 7.6640 - val_mean_absolute_error: 7.6640\n",
      "Epoch 178/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.2742 - mean_absolute_error: 3.2742 - val_loss: 7.6181 - val_mean_absolute_error: 7.6181\n",
      "Epoch 179/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.5909 - mean_absolute_error: 3.5909 - val_loss: 7.5185 - val_mean_absolute_error: 7.5185\n",
      "Epoch 180/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 3.0392 - mean_absolute_error: 3.0392 - val_loss: 7.3269 - val_mean_absolute_error: 7.3270\n",
      "Epoch 181/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.9122 - mean_absolute_error: 3.9122 - val_loss: 7.6002 - val_mean_absolute_error: 7.6002\n",
      "Epoch 182/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 4.6906 - mean_absolute_error: 4.6906 - val_loss: 7.8227 - val_mean_absolute_error: 7.8227\n",
      "Epoch 183/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.6179 - mean_absolute_error: 4.6179 - val_loss: 7.3590 - val_mean_absolute_error: 7.3590\n",
      "Epoch 184/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 4.0256 - mean_absolute_error: 4.0256 - val_loss: 7.5690 - val_mean_absolute_error: 7.5690\n",
      "Epoch 185/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.8137 - mean_absolute_error: 4.8137 - val_loss: 7.7277 - val_mean_absolute_error: 7.7277\n",
      "Epoch 186/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 3.5004 - mean_absolute_error: 3.5004 - val_loss: 7.7940 - val_mean_absolute_error: 7.7940\n",
      "Epoch 187/800\n",
      "1164/1164 [==============================] - 0s 131us/sample - loss: 4.0929 - mean_absolute_error: 4.0929 - val_loss: 7.4030 - val_mean_absolute_error: 7.4030\n",
      "Epoch 188/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 7.1578 - mean_absolute_error: 7.1578 - val_loss: 7.6362 - val_mean_absolute_error: 7.6362\n",
      "Epoch 189/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 8.0293 - mean_absolute_error: 8.0293 - val_loss: 7.5835 - val_mean_absolute_error: 7.5835\n",
      "Epoch 190/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 5.9468 - mean_absolute_error: 5.9468 - val_loss: 7.3269 - val_mean_absolute_error: 7.3269\n",
      "Epoch 191/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 3.0954 - mean_absolute_error: 3.0954 - val_loss: 7.8049 - val_mean_absolute_error: 7.8049\n",
      "Epoch 192/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 6.7910 - mean_absolute_error: 6.7910 - val_loss: 7.9767 - val_mean_absolute_error: 7.9767\n",
      "Epoch 193/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 10.3365 - mean_absolute_error: 10.3365 - val_loss: 7.6829 - val_mean_absolute_error: 7.6829\n",
      "Epoch 194/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 7.2948 - mean_absolute_error: 7.2948 - val_loss: 7.2791 - val_mean_absolute_error: 7.2791\n",
      "Epoch 195/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.9630 - mean_absolute_error: 4.9630 - val_loss: 7.7210 - val_mean_absolute_error: 7.7210\n",
      "Epoch 196/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.0961 - mean_absolute_error: 3.0961 - val_loss: 7.6242 - val_mean_absolute_error: 7.6242\n",
      "Epoch 197/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 4.0355 - mean_absolute_error: 4.0355 - val_loss: 7.4182 - val_mean_absolute_error: 7.4182\n",
      "Epoch 198/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 5.0566 - mean_absolute_error: 5.0566 - val_loss: 7.3708 - val_mean_absolute_error: 7.3708\n",
      "Epoch 199/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 4.1958 - mean_absolute_error: 4.1958 - val_loss: 7.7059 - val_mean_absolute_error: 7.7059\n",
      "Epoch 200/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.8004 - mean_absolute_error: 5.8004 - val_loss: 7.1368 - val_mean_absolute_error: 7.1368\n",
      "Epoch 201/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.9063 - mean_absolute_error: 3.9063 - val_loss: 7.3589 - val_mean_absolute_error: 7.3589\n",
      "Epoch 202/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.1131 - mean_absolute_error: 4.1131 - val_loss: 7.3621 - val_mean_absolute_error: 7.3621\n",
      "Epoch 203/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.9934 - mean_absolute_error: 2.9934 - val_loss: 7.5960 - val_mean_absolute_error: 7.5960\n",
      "Epoch 204/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.9043 - mean_absolute_error: 4.9043 - val_loss: 7.4841 - val_mean_absolute_error: 7.4841\n",
      "Epoch 205/800\n",
      "1164/1164 [==============================] - 0s 132us/sample - loss: 3.5293 - mean_absolute_error: 3.5293 - val_loss: 7.2712 - val_mean_absolute_error: 7.2712\n",
      "Epoch 206/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.4368 - mean_absolute_error: 3.4368 - val_loss: 7.4752 - val_mean_absolute_error: 7.4752\n",
      "Epoch 207/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.8239 - mean_absolute_error: 3.8239 - val_loss: 7.3787 - val_mean_absolute_error: 7.3787\n",
      "Epoch 208/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.5784 - mean_absolute_error: 3.5784 - val_loss: 7.7518 - val_mean_absolute_error: 7.7518\n",
      "Epoch 209/800\n",
      "1164/1164 [==============================] - 0s 158us/sample - loss: 5.6679 - mean_absolute_error: 5.6679 - val_loss: 7.5698 - val_mean_absolute_error: 7.5698\n",
      "Epoch 210/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.5810 - mean_absolute_error: 2.5810 - val_loss: 7.6060 - val_mean_absolute_error: 7.6060\n",
      "Epoch 211/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.5161 - mean_absolute_error: 2.5161 - val_loss: 7.6652 - val_mean_absolute_error: 7.6652\n",
      "Epoch 212/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.6043 - mean_absolute_error: 4.6043 - val_loss: 7.6404 - val_mean_absolute_error: 7.6404\n",
      "Epoch 213/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.0970 - mean_absolute_error: 4.0970 - val_loss: 7.6440 - val_mean_absolute_error: 7.6440\n",
      "Epoch 214/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.3827 - mean_absolute_error: 3.3827 - val_loss: 7.6936 - val_mean_absolute_error: 7.6936\n",
      "Epoch 215/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 11.3952 - mean_absolute_error: 11.3952 - val_loss: 7.0515 - val_mean_absolute_error: 7.0515\n",
      "Epoch 216/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 8.1038 - mean_absolute_error: 8.1038 - val_loss: 7.0584 - val_mean_absolute_error: 7.0584\n",
      "Epoch 217/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 4.7169 - mean_absolute_error: 4.7169 - val_loss: 7.5614 - val_mean_absolute_error: 7.5614\n",
      "Epoch 218/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.4258 - mean_absolute_error: 3.4258 - val_loss: 7.3270 - val_mean_absolute_error: 7.3270\n",
      "Epoch 219/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 5.7635 - mean_absolute_error: 5.7635 - val_loss: 7.8740 - val_mean_absolute_error: 7.8740\n",
      "Epoch 220/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 4.5262 - mean_absolute_error: 4.5262 - val_loss: 7.7267 - val_mean_absolute_error: 7.7267\n",
      "Epoch 221/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.2989 - mean_absolute_error: 4.2989 - val_loss: 7.3038 - val_mean_absolute_error: 7.3038\n",
      "Epoch 222/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.3521 - mean_absolute_error: 3.3521 - val_loss: 7.1935 - val_mean_absolute_error: 7.1935\n",
      "Epoch 223/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 7.3436 - mean_absolute_error: 7.3436 - val_loss: 7.5936 - val_mean_absolute_error: 7.5936\n",
      "Epoch 224/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.4443 - mean_absolute_error: 3.4443 - val_loss: 7.4709 - val_mean_absolute_error: 7.4709\n",
      "Epoch 225/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.0478 - mean_absolute_error: 3.0478 - val_loss: 7.5927 - val_mean_absolute_error: 7.5927\n",
      "Epoch 226/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5180 - mean_absolute_error: 3.5180 - val_loss: 7.8554 - val_mean_absolute_error: 7.8554\n",
      "Epoch 227/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 9.3832 - mean_absolute_error: 9.3832 - val_loss: 7.1325 - val_mean_absolute_error: 7.1325\n",
      "Epoch 228/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0373 - mean_absolute_error: 4.0373 - val_loss: 7.6502 - val_mean_absolute_error: 7.6502\n",
      "Epoch 229/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.3457 - mean_absolute_error: 2.3457 - val_loss: 7.6163 - val_mean_absolute_error: 7.6163\n",
      "Epoch 230/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.0886 - mean_absolute_error: 4.0886 - val_loss: 7.0095 - val_mean_absolute_error: 7.0095\n",
      "Epoch 231/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 6.0580 - mean_absolute_error: 6.0580 - val_loss: 7.7924 - val_mean_absolute_error: 7.7924\n",
      "Epoch 232/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.5675 - mean_absolute_error: 4.5675 - val_loss: 7.4937 - val_mean_absolute_error: 7.4937\n",
      "Epoch 233/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.9407 - mean_absolute_error: 3.9407 - val_loss: 7.7154 - val_mean_absolute_error: 7.7154\n",
      "Epoch 234/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.2546 - mean_absolute_error: 5.2546 - val_loss: 7.5505 - val_mean_absolute_error: 7.5505\n",
      "Epoch 235/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.2838 - mean_absolute_error: 3.2838 - val_loss: 7.7007 - val_mean_absolute_error: 7.7007\n",
      "Epoch 236/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.7304 - mean_absolute_error: 2.7304 - val_loss: 7.5068 - val_mean_absolute_error: 7.5068\n",
      "Epoch 237/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 2.5882 - mean_absolute_error: 2.5882 - val_loss: 7.7239 - val_mean_absolute_error: 7.7239\n",
      "Epoch 238/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.0676 - mean_absolute_error: 4.0676 - val_loss: 7.8954 - val_mean_absolute_error: 7.8954\n",
      "Epoch 239/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.0623 - mean_absolute_error: 3.0623 - val_loss: 7.4932 - val_mean_absolute_error: 7.4932\n",
      "Epoch 240/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 5.5548 - mean_absolute_error: 5.5548 - val_loss: 7.5417 - val_mean_absolute_error: 7.5417\n",
      "Epoch 241/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.7117 - mean_absolute_error: 4.7117 - val_loss: 7.9157 - val_mean_absolute_error: 7.9157\n",
      "Epoch 242/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.2661 - mean_absolute_error: 5.2661 - val_loss: 7.4477 - val_mean_absolute_error: 7.4477\n",
      "Epoch 243/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.1648 - mean_absolute_error: 2.1648 - val_loss: 7.4163 - val_mean_absolute_error: 7.4163\n",
      "Epoch 244/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.4281 - mean_absolute_error: 3.4281 - val_loss: 7.2320 - val_mean_absolute_error: 7.2320\n",
      "Epoch 245/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 7.8990 - mean_absolute_error: 7.8990 - val_loss: 7.4978 - val_mean_absolute_error: 7.4978\n",
      "Epoch 246/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 7.3610 - mean_absolute_error: 7.3610 - val_loss: 7.3177 - val_mean_absolute_error: 7.3177\n",
      "Epoch 247/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.7470 - mean_absolute_error: 3.7470 - val_loss: 7.4968 - val_mean_absolute_error: 7.4968\n",
      "Epoch 248/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 8.2603 - mean_absolute_error: 8.2603 - val_loss: 7.4388 - val_mean_absolute_error: 7.4388\n",
      "Epoch 249/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.8881 - mean_absolute_error: 3.8881 - val_loss: 7.7811 - val_mean_absolute_error: 7.7811\n",
      "Epoch 250/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.5072 - mean_absolute_error: 4.5072 - val_loss: 7.2344 - val_mean_absolute_error: 7.2344\n",
      "Epoch 251/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 7.1270 - mean_absolute_error: 7.1270 - val_loss: 7.4131 - val_mean_absolute_error: 7.4131\n",
      "Epoch 252/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.7091 - mean_absolute_error: 4.7091 - val_loss: 7.4788 - val_mean_absolute_error: 7.4788\n",
      "Epoch 253/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.8410 - mean_absolute_error: 5.8410 - val_loss: 7.2580 - val_mean_absolute_error: 7.2580\n",
      "Epoch 254/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.3438 - mean_absolute_error: 4.3438 - val_loss: 7.2322 - val_mean_absolute_error: 7.2322\n",
      "Epoch 255/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.0052 - mean_absolute_error: 4.0052 - val_loss: 7.4978 - val_mean_absolute_error: 7.4978\n",
      "Epoch 256/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 5.0510 - mean_absolute_error: 5.0510 - val_loss: 7.2355 - val_mean_absolute_error: 7.2355\n",
      "Epoch 257/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.4562 - mean_absolute_error: 3.4562 - val_loss: 7.1100 - val_mean_absolute_error: 7.1100\n",
      "Epoch 258/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 5.4144 - mean_absolute_error: 5.4144 - val_loss: 7.3761 - val_mean_absolute_error: 7.3761\n",
      "Epoch 259/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.0290 - mean_absolute_error: 3.0290 - val_loss: 7.8127 - val_mean_absolute_error: 7.8127\n",
      "Epoch 260/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 2.6090 - mean_absolute_error: 2.6090 - val_loss: 7.4605 - val_mean_absolute_error: 7.4605\n",
      "Epoch 261/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.6536 - mean_absolute_error: 3.6536 - val_loss: 7.2548 - val_mean_absolute_error: 7.2548\n",
      "Epoch 262/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.9413 - mean_absolute_error: 3.9413 - val_loss: 7.5496 - val_mean_absolute_error: 7.5496\n",
      "Epoch 263/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 6.5858 - mean_absolute_error: 6.5858 - val_loss: 7.2749 - val_mean_absolute_error: 7.2749\n",
      "Epoch 264/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.3704 - mean_absolute_error: 5.3704 - val_loss: 7.1462 - val_mean_absolute_error: 7.1462\n",
      "Epoch 265/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0279 - mean_absolute_error: 4.0279 - val_loss: 7.6681 - val_mean_absolute_error: 7.6681\n",
      "Epoch 266/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.9010 - mean_absolute_error: 2.9010 - val_loss: 7.3328 - val_mean_absolute_error: 7.3328\n",
      "Epoch 267/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.3915 - mean_absolute_error: 3.3915 - val_loss: 7.7888 - val_mean_absolute_error: 7.7888\n",
      "Epoch 268/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.4299 - mean_absolute_error: 3.4299 - val_loss: 7.5700 - val_mean_absolute_error: 7.5700\n",
      "Epoch 269/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.7756 - mean_absolute_error: 4.7756 - val_loss: 7.5174 - val_mean_absolute_error: 7.5174\n",
      "Epoch 270/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.4095 - mean_absolute_error: 4.4095 - val_loss: 7.6231 - val_mean_absolute_error: 7.6231\n",
      "Epoch 271/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 3.7417 - mean_absolute_error: 3.7417 - val_loss: 7.8864 - val_mean_absolute_error: 7.8864\n",
      "Epoch 272/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.2359 - mean_absolute_error: 3.2359 - val_loss: 7.5027 - val_mean_absolute_error: 7.5027\n",
      "Epoch 273/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.4051 - mean_absolute_error: 2.4051 - val_loss: 7.5785 - val_mean_absolute_error: 7.5785\n",
      "Epoch 274/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.0867 - mean_absolute_error: 3.0867 - val_loss: 7.3493 - val_mean_absolute_error: 7.3493\n",
      "Epoch 275/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 3.8937 - mean_absolute_error: 3.8937 - val_loss: 7.5147 - val_mean_absolute_error: 7.5147\n",
      "Epoch 276/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.7229 - mean_absolute_error: 2.7229 - val_loss: 7.5063 - val_mean_absolute_error: 7.5063\n",
      "Epoch 277/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 6.8927 - mean_absolute_error: 6.8927 - val_loss: 7.4215 - val_mean_absolute_error: 7.4215\n",
      "Epoch 278/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 4.5181 - mean_absolute_error: 4.5181 - val_loss: 7.7070 - val_mean_absolute_error: 7.7070\n",
      "Epoch 279/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.5737 - mean_absolute_error: 5.5737 - val_loss: 7.5718 - val_mean_absolute_error: 7.5718\n",
      "Epoch 280/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.4165 - mean_absolute_error: 3.4165 - val_loss: 7.7440 - val_mean_absolute_error: 7.7440\n",
      "Epoch 281/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 2.6548 - mean_absolute_error: 2.6548 - val_loss: 7.5185 - val_mean_absolute_error: 7.5185\n",
      "Epoch 282/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 3.6753 - mean_absolute_error: 3.6753 - val_loss: 7.3135 - val_mean_absolute_error: 7.3135\n",
      "Epoch 283/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 5.8623 - mean_absolute_error: 5.8623 - val_loss: 7.1773 - val_mean_absolute_error: 7.1773\n",
      "Epoch 284/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.1566 - mean_absolute_error: 3.1566 - val_loss: 7.3890 - val_mean_absolute_error: 7.3890\n",
      "Epoch 285/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.2966 - mean_absolute_error: 3.2966 - val_loss: 7.6954 - val_mean_absolute_error: 7.6954\n",
      "Epoch 286/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.7014 - mean_absolute_error: 5.7014 - val_loss: 7.2761 - val_mean_absolute_error: 7.2761\n",
      "Epoch 287/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.1254 - mean_absolute_error: 3.1254 - val_loss: 7.3233 - val_mean_absolute_error: 7.3233\n",
      "Epoch 288/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.7965 - mean_absolute_error: 2.7965 - val_loss: 7.4093 - val_mean_absolute_error: 7.4093\n",
      "Epoch 289/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 4.5592 - mean_absolute_error: 4.5592 - val_loss: 7.1831 - val_mean_absolute_error: 7.1831\n",
      "Epoch 290/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.7925 - mean_absolute_error: 3.7925 - val_loss: 7.7726 - val_mean_absolute_error: 7.7726\n",
      "Epoch 291/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.2964 - mean_absolute_error: 2.2964 - val_loss: 7.4915 - val_mean_absolute_error: 7.4915\n",
      "Epoch 292/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 1.9129 - mean_absolute_error: 1.9129 - val_loss: 7.4909 - val_mean_absolute_error: 7.4909\n",
      "Epoch 293/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.8032 - mean_absolute_error: 3.8032 - val_loss: 7.2638 - val_mean_absolute_error: 7.2638\n",
      "Epoch 294/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.2433 - mean_absolute_error: 2.2433 - val_loss: 7.5590 - val_mean_absolute_error: 7.5590\n",
      "Epoch 295/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.3247 - mean_absolute_error: 3.3247 - val_loss: 7.2965 - val_mean_absolute_error: 7.2965\n",
      "Epoch 296/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.8936 - mean_absolute_error: 3.8936 - val_loss: 7.3305 - val_mean_absolute_error: 7.3305\n",
      "Epoch 297/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 7.9089 - mean_absolute_error: 7.9089 - val_loss: 7.1761 - val_mean_absolute_error: 7.1761\n",
      "Epoch 298/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.1285 - mean_absolute_error: 4.1285 - val_loss: 7.4765 - val_mean_absolute_error: 7.4765\n",
      "Epoch 299/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 3.8929 - mean_absolute_error: 3.8929 - val_loss: 7.1615 - val_mean_absolute_error: 7.1615\n",
      "Epoch 300/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.5500 - mean_absolute_error: 3.5500 - val_loss: 7.6040 - val_mean_absolute_error: 7.6040\n",
      "Epoch 301/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 5.5678 - mean_absolute_error: 5.5678 - val_loss: 7.3170 - val_mean_absolute_error: 7.3170\n",
      "Epoch 302/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.8750 - mean_absolute_error: 4.8750 - val_loss: 7.2096 - val_mean_absolute_error: 7.2096\n",
      "Epoch 303/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.2819 - mean_absolute_error: 2.2819 - val_loss: 7.5386 - val_mean_absolute_error: 7.5386\n",
      "Epoch 304/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.6769 - mean_absolute_error: 2.6769 - val_loss: 7.6546 - val_mean_absolute_error: 7.6546\n",
      "Epoch 305/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.0021 - mean_absolute_error: 4.0021 - val_loss: 7.6563 - val_mean_absolute_error: 7.6563\n",
      "Epoch 306/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.4758 - mean_absolute_error: 3.4758 - val_loss: 7.2757 - val_mean_absolute_error: 7.2757\n",
      "Epoch 307/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.6732 - mean_absolute_error: 4.6732 - val_loss: 7.3344 - val_mean_absolute_error: 7.3344\n",
      "Epoch 308/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.9132 - mean_absolute_error: 3.9132 - val_loss: 7.3437 - val_mean_absolute_error: 7.3437\n",
      "Epoch 309/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 3.1725 - mean_absolute_error: 3.1725 - val_loss: 7.1334 - val_mean_absolute_error: 7.1334\n",
      "Epoch 310/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.2297 - mean_absolute_error: 2.2297 - val_loss: 7.4080 - val_mean_absolute_error: 7.4080\n",
      "Epoch 311/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.3790 - mean_absolute_error: 3.3790 - val_loss: 7.1579 - val_mean_absolute_error: 7.1579\n",
      "Epoch 312/800\n",
      "1164/1164 [==============================] - 0s 155us/sample - loss: 2.6566 - mean_absolute_error: 2.6566 - val_loss: 7.4660 - val_mean_absolute_error: 7.4660\n",
      "Epoch 313/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.6381 - mean_absolute_error: 4.6381 - val_loss: 7.5624 - val_mean_absolute_error: 7.5624\n",
      "Epoch 314/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.7245 - mean_absolute_error: 2.7245 - val_loss: 7.2503 - val_mean_absolute_error: 7.2503\n",
      "Epoch 315/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.8644 - mean_absolute_error: 2.8644 - val_loss: 7.0587 - val_mean_absolute_error: 7.0587\n",
      "Epoch 316/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.2538 - mean_absolute_error: 4.2538 - val_loss: 7.2057 - val_mean_absolute_error: 7.2057\n",
      "Epoch 317/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.1070 - mean_absolute_error: 4.1070 - val_loss: 7.6478 - val_mean_absolute_error: 7.6478\n",
      "Epoch 318/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 4.3573 - mean_absolute_error: 4.3573 - val_loss: 7.2730 - val_mean_absolute_error: 7.2730\n",
      "Epoch 319/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.8760 - mean_absolute_error: 3.8760 - val_loss: 7.4930 - val_mean_absolute_error: 7.4930\n",
      "Epoch 320/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 1.9826 - mean_absolute_error: 1.9826 - val_loss: 7.3035 - val_mean_absolute_error: 7.3035\n",
      "Epoch 321/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.9493 - mean_absolute_error: 3.9493 - val_loss: 7.5191 - val_mean_absolute_error: 7.5191\n",
      "Epoch 322/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.5582 - mean_absolute_error: 4.5582 - val_loss: 7.6341 - val_mean_absolute_error: 7.6341\n",
      "Epoch 323/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 3.3446 - mean_absolute_error: 3.3446 - val_loss: 7.5438 - val_mean_absolute_error: 7.5438\n",
      "Epoch 324/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.4546 - mean_absolute_error: 4.4546 - val_loss: 7.6492 - val_mean_absolute_error: 7.6492\n",
      "Epoch 325/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.6785 - mean_absolute_error: 4.6785 - val_loss: 7.2652 - val_mean_absolute_error: 7.2652\n",
      "Epoch 326/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.6675 - mean_absolute_error: 4.6675 - val_loss: 7.2825 - val_mean_absolute_error: 7.2825\n",
      "Epoch 327/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.0139 - mean_absolute_error: 4.0139 - val_loss: 7.5781 - val_mean_absolute_error: 7.5781\n",
      "Epoch 328/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 2.7902 - mean_absolute_error: 2.7902 - val_loss: 7.4366 - val_mean_absolute_error: 7.4366\n",
      "Epoch 329/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.8040 - mean_absolute_error: 3.8040 - val_loss: 7.7935 - val_mean_absolute_error: 7.7935\n",
      "Epoch 330/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.9680 - mean_absolute_error: 4.9680 - val_loss: 7.6510 - val_mean_absolute_error: 7.6510\n",
      "Epoch 331/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.7554 - mean_absolute_error: 2.7554 - val_loss: 7.2977 - val_mean_absolute_error: 7.2977\n",
      "Epoch 332/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.9590 - mean_absolute_error: 2.9590 - val_loss: 7.3249 - val_mean_absolute_error: 7.3249\n",
      "Epoch 333/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 6.1184 - mean_absolute_error: 6.1184 - val_loss: 7.4219 - val_mean_absolute_error: 7.4219\n",
      "Epoch 334/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.2482 - mean_absolute_error: 4.2482 - val_loss: 7.5459 - val_mean_absolute_error: 7.5459\n",
      "Epoch 335/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.8301 - mean_absolute_error: 4.8301 - val_loss: 6.8506 - val_mean_absolute_error: 6.8506\n",
      "Epoch 336/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 4.9210 - mean_absolute_error: 4.9210 - val_loss: 7.4154 - val_mean_absolute_error: 7.4154\n",
      "Epoch 337/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.2680 - mean_absolute_error: 2.2680 - val_loss: 7.7207 - val_mean_absolute_error: 7.7207\n",
      "Epoch 338/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.7775 - mean_absolute_error: 3.7775 - val_loss: 7.9297 - val_mean_absolute_error: 7.9297\n",
      "Epoch 339/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 6.2864 - mean_absolute_error: 6.2864 - val_loss: 7.2168 - val_mean_absolute_error: 7.2168\n",
      "Epoch 340/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.3593 - mean_absolute_error: 4.3593 - val_loss: 7.3613 - val_mean_absolute_error: 7.3613\n",
      "Epoch 341/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 4.7116 - mean_absolute_error: 4.7116 - val_loss: 7.7290 - val_mean_absolute_error: 7.7290\n",
      "Epoch 342/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.7467 - mean_absolute_error: 4.7467 - val_loss: 7.4748 - val_mean_absolute_error: 7.4748\n",
      "Epoch 343/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.9407 - mean_absolute_error: 2.9407 - val_loss: 7.1524 - val_mean_absolute_error: 7.1524\n",
      "Epoch 344/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.9681 - mean_absolute_error: 2.9681 - val_loss: 7.4272 - val_mean_absolute_error: 7.4272\n",
      "Epoch 345/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.7911 - mean_absolute_error: 2.7911 - val_loss: 7.5567 - val_mean_absolute_error: 7.5567\n",
      "Epoch 346/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.9112 - mean_absolute_error: 3.9112 - val_loss: 7.5819 - val_mean_absolute_error: 7.5819\n",
      "Epoch 347/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 3.0861 - mean_absolute_error: 3.0861 - val_loss: 7.1340 - val_mean_absolute_error: 7.1340\n",
      "Epoch 348/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.0213 - mean_absolute_error: 4.0213 - val_loss: 7.2343 - val_mean_absolute_error: 7.2343\n",
      "Epoch 349/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.2429 - mean_absolute_error: 2.2429 - val_loss: 7.6704 - val_mean_absolute_error: 7.6704\n",
      "Epoch 350/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.2216 - mean_absolute_error: 3.2216 - val_loss: 7.8921 - val_mean_absolute_error: 7.8921\n",
      "Epoch 351/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 9.5041 - mean_absolute_error: 9.5041 - val_loss: 7.1758 - val_mean_absolute_error: 7.1758\n",
      "Epoch 352/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 9.0700 - mean_absolute_error: 9.0700 - val_loss: 7.8929 - val_mean_absolute_error: 7.8929\n",
      "Epoch 353/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.5896 - mean_absolute_error: 4.5896 - val_loss: 7.2080 - val_mean_absolute_error: 7.2080\n",
      "Epoch 354/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 8.3491 - mean_absolute_error: 8.3491 - val_loss: 7.6575 - val_mean_absolute_error: 7.6575\n",
      "Epoch 355/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 7.8180 - mean_absolute_error: 7.8180 - val_loss: 8.1026 - val_mean_absolute_error: 8.1026\n",
      "Epoch 356/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.5082 - mean_absolute_error: 5.5082 - val_loss: 7.1924 - val_mean_absolute_error: 7.1924\n",
      "Epoch 357/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 4.1630 - mean_absolute_error: 4.1630 - val_loss: 7.7513 - val_mean_absolute_error: 7.7513\n",
      "Epoch 358/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.8832 - mean_absolute_error: 2.8832 - val_loss: 7.1015 - val_mean_absolute_error: 7.1015\n",
      "Epoch 359/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.1961 - mean_absolute_error: 3.1961 - val_loss: 7.6398 - val_mean_absolute_error: 7.6398\n",
      "Epoch 360/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.9990 - mean_absolute_error: 4.9990 - val_loss: 7.7162 - val_mean_absolute_error: 7.7162\n",
      "Epoch 361/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.3576 - mean_absolute_error: 5.3576 - val_loss: 6.9941 - val_mean_absolute_error: 6.9941\n",
      "Epoch 362/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.7248 - mean_absolute_error: 3.7248 - val_loss: 7.3093 - val_mean_absolute_error: 7.3093\n",
      "Epoch 363/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 2.7167 - mean_absolute_error: 2.7167 - val_loss: 7.1929 - val_mean_absolute_error: 7.1929\n",
      "Epoch 364/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.9041 - mean_absolute_error: 2.9041 - val_loss: 7.6464 - val_mean_absolute_error: 7.6464\n",
      "Epoch 365/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 3.0650 - mean_absolute_error: 3.0650 - val_loss: 7.6892 - val_mean_absolute_error: 7.6892\n",
      "Epoch 366/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.6180 - mean_absolute_error: 3.6180 - val_loss: 7.7815 - val_mean_absolute_error: 7.7815\n",
      "Epoch 367/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.5760 - mean_absolute_error: 3.5760 - val_loss: 7.6282 - val_mean_absolute_error: 7.6282\n",
      "Epoch 368/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.6792 - mean_absolute_error: 2.6792 - val_loss: 7.3180 - val_mean_absolute_error: 7.3180\n",
      "Epoch 369/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.0501 - mean_absolute_error: 2.0501 - val_loss: 7.5581 - val_mean_absolute_error: 7.5581\n",
      "Epoch 370/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.6855 - mean_absolute_error: 3.6855 - val_loss: 7.9411 - val_mean_absolute_error: 7.9411\n",
      "Epoch 371/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0932 - mean_absolute_error: 4.0932 - val_loss: 7.3215 - val_mean_absolute_error: 7.3215\n",
      "Epoch 372/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.3213 - mean_absolute_error: 3.3213 - val_loss: 7.9709 - val_mean_absolute_error: 7.9709\n",
      "Epoch 373/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 6.9905 - mean_absolute_error: 6.9905 - val_loss: 7.2109 - val_mean_absolute_error: 7.2109\n",
      "Epoch 374/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.8493 - mean_absolute_error: 2.8493 - val_loss: 7.5035 - val_mean_absolute_error: 7.5035\n",
      "Epoch 375/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.9123 - mean_absolute_error: 3.9123 - val_loss: 7.4310 - val_mean_absolute_error: 7.4310\n",
      "Epoch 376/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.6717 - mean_absolute_error: 3.6717 - val_loss: 7.3619 - val_mean_absolute_error: 7.3619\n",
      "Epoch 377/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.8811 - mean_absolute_error: 2.8811 - val_loss: 7.3958 - val_mean_absolute_error: 7.3958\n",
      "Epoch 378/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.1869 - mean_absolute_error: 2.1869 - val_loss: 7.2189 - val_mean_absolute_error: 7.2189\n",
      "Epoch 379/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.6606 - mean_absolute_error: 2.6606 - val_loss: 7.4425 - val_mean_absolute_error: 7.4425\n",
      "Epoch 380/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.7818 - mean_absolute_error: 4.7818 - val_loss: 7.1143 - val_mean_absolute_error: 7.1143\n",
      "Epoch 381/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 6.9124 - mean_absolute_error: 6.9124 - val_loss: 7.7848 - val_mean_absolute_error: 7.7848\n",
      "Epoch 382/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.7851 - mean_absolute_error: 5.7851 - val_loss: 7.0821 - val_mean_absolute_error: 7.0821\n",
      "Epoch 383/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.7765 - mean_absolute_error: 4.7765 - val_loss: 7.3249 - val_mean_absolute_error: 7.3249\n",
      "Epoch 384/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.7193 - mean_absolute_error: 3.7193 - val_loss: 7.3487 - val_mean_absolute_error: 7.3487\n",
      "Epoch 385/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.0761 - mean_absolute_error: 3.0761 - val_loss: 7.5884 - val_mean_absolute_error: 7.5884\n",
      "Epoch 386/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.7353 - mean_absolute_error: 3.7353 - val_loss: 7.4578 - val_mean_absolute_error: 7.4578\n",
      "Epoch 387/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 3.4106 - mean_absolute_error: 3.4106 - val_loss: 7.5992 - val_mean_absolute_error: 7.5992\n",
      "Epoch 388/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.6274 - mean_absolute_error: 4.6274 - val_loss: 7.3817 - val_mean_absolute_error: 7.3817\n",
      "Epoch 389/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.2676 - mean_absolute_error: 3.2676 - val_loss: 7.3887 - val_mean_absolute_error: 7.3887\n",
      "Epoch 390/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.3011 - mean_absolute_error: 2.3011 - val_loss: 7.6085 - val_mean_absolute_error: 7.6085\n",
      "Epoch 391/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.1817 - mean_absolute_error: 4.1817 - val_loss: 7.1481 - val_mean_absolute_error: 7.1481\n",
      "Epoch 392/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.3434 - mean_absolute_error: 3.3434 - val_loss: 7.9998 - val_mean_absolute_error: 7.9998\n",
      "Epoch 393/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 5.1419 - mean_absolute_error: 5.1419 - val_loss: 7.4921 - val_mean_absolute_error: 7.4921\n",
      "Epoch 394/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.8505 - mean_absolute_error: 2.8505 - val_loss: 7.4563 - val_mean_absolute_error: 7.4563\n",
      "Epoch 395/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.2230 - mean_absolute_error: 2.2230 - val_loss: 7.1850 - val_mean_absolute_error: 7.1850\n",
      "Epoch 396/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.3278 - mean_absolute_error: 3.3278 - val_loss: 7.4819 - val_mean_absolute_error: 7.4819\n",
      "Epoch 397/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.4101 - mean_absolute_error: 2.4101 - val_loss: 7.5865 - val_mean_absolute_error: 7.5865\n",
      "Epoch 398/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.8185 - mean_absolute_error: 2.8185 - val_loss: 7.5198 - val_mean_absolute_error: 7.5198\n",
      "Epoch 399/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.4455 - mean_absolute_error: 4.4455 - val_loss: 7.2794 - val_mean_absolute_error: 7.2794\n",
      "Epoch 400/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 3.2708 - mean_absolute_error: 3.2708 - val_loss: 7.3196 - val_mean_absolute_error: 7.3196\n",
      "Epoch 401/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 3.9939 - mean_absolute_error: 3.9939 - val_loss: 7.3640 - val_mean_absolute_error: 7.3640\n",
      "Epoch 402/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0375 - mean_absolute_error: 4.0375 - val_loss: 7.4267 - val_mean_absolute_error: 7.4267\n",
      "Epoch 403/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.6129 - mean_absolute_error: 2.6129 - val_loss: 7.3952 - val_mean_absolute_error: 7.3952\n",
      "Epoch 404/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.2353 - mean_absolute_error: 3.2353 - val_loss: 7.6683 - val_mean_absolute_error: 7.6683\n",
      "Epoch 405/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.5875 - mean_absolute_error: 3.5875 - val_loss: 7.1449 - val_mean_absolute_error: 7.1449\n",
      "Epoch 406/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 3.3763 - mean_absolute_error: 3.3763 - val_loss: 7.3649 - val_mean_absolute_error: 7.3649\n",
      "Epoch 407/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 3.6806 - mean_absolute_error: 3.6806 - val_loss: 7.3323 - val_mean_absolute_error: 7.3323\n",
      "Epoch 408/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 3.5693 - mean_absolute_error: 3.5693 - val_loss: 7.4758 - val_mean_absolute_error: 7.4758\n",
      "Epoch 409/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.6894 - mean_absolute_error: 2.6894 - val_loss: 7.2449 - val_mean_absolute_error: 7.2449\n",
      "Epoch 410/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.6113 - mean_absolute_error: 3.6113 - val_loss: 7.1313 - val_mean_absolute_error: 7.1313\n",
      "Epoch 411/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.8921 - mean_absolute_error: 1.8921 - val_loss: 7.3989 - val_mean_absolute_error: 7.3989\n",
      "Epoch 412/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 5.0952 - mean_absolute_error: 5.0952 - val_loss: 7.4349 - val_mean_absolute_error: 7.4349\n",
      "Epoch 413/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 5.2581 - mean_absolute_error: 5.2581 - val_loss: 7.1641 - val_mean_absolute_error: 7.1641\n",
      "Epoch 414/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.4492 - mean_absolute_error: 5.4492 - val_loss: 7.2056 - val_mean_absolute_error: 7.2056\n",
      "Epoch 415/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 3.2809 - mean_absolute_error: 3.2809 - val_loss: 7.0720 - val_mean_absolute_error: 7.0720\n",
      "Epoch 416/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 4.3340 - mean_absolute_error: 4.3340 - val_loss: 7.1624 - val_mean_absolute_error: 7.1624\n",
      "Epoch 417/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 3.4659 - mean_absolute_error: 3.4659 - val_loss: 7.6131 - val_mean_absolute_error: 7.6131\n",
      "Epoch 418/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.5595 - mean_absolute_error: 3.5595 - val_loss: 7.6445 - val_mean_absolute_error: 7.6445\n",
      "Epoch 419/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.3646 - mean_absolute_error: 2.3646 - val_loss: 7.3938 - val_mean_absolute_error: 7.3938\n",
      "Epoch 420/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.9175 - mean_absolute_error: 3.9175 - val_loss: 7.1928 - val_mean_absolute_error: 7.1928\n",
      "Epoch 421/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.9292 - mean_absolute_error: 3.9292 - val_loss: 7.3661 - val_mean_absolute_error: 7.3661\n",
      "Epoch 422/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.8355 - mean_absolute_error: 2.8355 - val_loss: 7.3697 - val_mean_absolute_error: 7.3697\n",
      "Epoch 423/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.1222 - mean_absolute_error: 3.1222 - val_loss: 7.4152 - val_mean_absolute_error: 7.4152\n",
      "Epoch 424/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.5763 - mean_absolute_error: 3.5763 - val_loss: 7.7425 - val_mean_absolute_error: 7.7425\n",
      "Epoch 425/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 5.2393 - mean_absolute_error: 5.2393 - val_loss: 7.2893 - val_mean_absolute_error: 7.2893\n",
      "Epoch 426/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.9933 - mean_absolute_error: 2.9933 - val_loss: 7.3726 - val_mean_absolute_error: 7.3726\n",
      "Epoch 427/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.5003 - mean_absolute_error: 2.5003 - val_loss: 7.4553 - val_mean_absolute_error: 7.4553\n",
      "Epoch 428/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 2.7516 - mean_absolute_error: 2.7516 - val_loss: 7.3092 - val_mean_absolute_error: 7.3092\n",
      "Epoch 429/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.9358 - mean_absolute_error: 2.9358 - val_loss: 7.3589 - val_mean_absolute_error: 7.3589\n",
      "Epoch 430/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 3.6163 - mean_absolute_error: 3.6163 - val_loss: 7.4082 - val_mean_absolute_error: 7.4082\n",
      "Epoch 431/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.0147 - mean_absolute_error: 5.0147 - val_loss: 7.0092 - val_mean_absolute_error: 7.0092\n",
      "Epoch 432/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.1158 - mean_absolute_error: 3.1158 - val_loss: 7.3914 - val_mean_absolute_error: 7.3914\n",
      "Epoch 433/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.7745 - mean_absolute_error: 2.7745 - val_loss: 7.3880 - val_mean_absolute_error: 7.3880\n",
      "Epoch 434/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.8508 - mean_absolute_error: 2.8508 - val_loss: 7.2981 - val_mean_absolute_error: 7.2981\n",
      "Epoch 435/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.2175 - mean_absolute_error: 4.2175 - val_loss: 7.3680 - val_mean_absolute_error: 7.3680\n",
      "Epoch 436/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.4167 - mean_absolute_error: 4.4167 - val_loss: 6.9042 - val_mean_absolute_error: 6.9042\n",
      "Epoch 437/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 1.8783 - mean_absolute_error: 1.8783 - val_loss: 7.6028 - val_mean_absolute_error: 7.6028\n",
      "Epoch 438/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.7755 - mean_absolute_error: 4.7755 - val_loss: 7.1724 - val_mean_absolute_error: 7.1724\n",
      "Epoch 439/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.6012 - mean_absolute_error: 3.6012 - val_loss: 7.7498 - val_mean_absolute_error: 7.7498\n",
      "Epoch 440/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.5944 - mean_absolute_error: 3.5944 - val_loss: 7.1872 - val_mean_absolute_error: 7.1872\n",
      "Epoch 441/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5030 - mean_absolute_error: 3.5030 - val_loss: 7.4070 - val_mean_absolute_error: 7.4070\n",
      "Epoch 442/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.2097 - mean_absolute_error: 5.2097 - val_loss: 6.9913 - val_mean_absolute_error: 6.9913\n",
      "Epoch 443/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.9417 - mean_absolute_error: 2.9417 - val_loss: 7.1391 - val_mean_absolute_error: 7.1391\n",
      "Epoch 444/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.1764 - mean_absolute_error: 4.1764 - val_loss: 7.1409 - val_mean_absolute_error: 7.1409\n",
      "Epoch 445/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.8641 - mean_absolute_error: 4.8641 - val_loss: 7.3628 - val_mean_absolute_error: 7.3628\n",
      "Epoch 446/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.7445 - mean_absolute_error: 2.7445 - val_loss: 7.0842 - val_mean_absolute_error: 7.0842\n",
      "Epoch 447/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 5.3057 - mean_absolute_error: 5.3057 - val_loss: 7.0822 - val_mean_absolute_error: 7.0822\n",
      "Epoch 448/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.8353 - mean_absolute_error: 3.8353 - val_loss: 7.5377 - val_mean_absolute_error: 7.5377\n",
      "Epoch 449/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.4104 - mean_absolute_error: 3.4104 - val_loss: 7.0668 - val_mean_absolute_error: 7.0668\n",
      "Epoch 450/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.2643 - mean_absolute_error: 4.2643 - val_loss: 7.1459 - val_mean_absolute_error: 7.1459\n",
      "Epoch 451/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.6561 - mean_absolute_error: 2.6561 - val_loss: 7.2390 - val_mean_absolute_error: 7.2390\n",
      "Epoch 452/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.7948 - mean_absolute_error: 2.7948 - val_loss: 7.3166 - val_mean_absolute_error: 7.3166\n",
      "Epoch 453/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.3178 - mean_absolute_error: 2.3178 - val_loss: 7.2339 - val_mean_absolute_error: 7.2339\n",
      "Epoch 454/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.3825 - mean_absolute_error: 2.3825 - val_loss: 7.3510 - val_mean_absolute_error: 7.3510\n",
      "Epoch 455/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.7014 - mean_absolute_error: 2.7014 - val_loss: 7.3291 - val_mean_absolute_error: 7.3291\n",
      "Epoch 456/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.6105 - mean_absolute_error: 2.6105 - val_loss: 7.3412 - val_mean_absolute_error: 7.3412\n",
      "Epoch 457/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.6936 - mean_absolute_error: 2.6936 - val_loss: 6.9340 - val_mean_absolute_error: 6.9340\n",
      "Epoch 458/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.0975 - mean_absolute_error: 2.0975 - val_loss: 7.5617 - val_mean_absolute_error: 7.5617\n",
      "Epoch 459/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.2126 - mean_absolute_error: 2.2126 - val_loss: 7.2446 - val_mean_absolute_error: 7.2446\n",
      "Epoch 460/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 5.5516 - mean_absolute_error: 5.5516 - val_loss: 7.5885 - val_mean_absolute_error: 7.5885\n",
      "Epoch 461/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 7.0036 - mean_absolute_error: 7.0036 - val_loss: 7.3916 - val_mean_absolute_error: 7.3916\n",
      "Epoch 462/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.3267 - mean_absolute_error: 4.3267 - val_loss: 7.0358 - val_mean_absolute_error: 7.0358\n",
      "Epoch 463/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 5.3196 - mean_absolute_error: 5.3196 - val_loss: 7.1399 - val_mean_absolute_error: 7.1399\n",
      "Epoch 464/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.3701 - mean_absolute_error: 3.3701 - val_loss: 7.0139 - val_mean_absolute_error: 7.0139\n",
      "Epoch 465/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.3300 - mean_absolute_error: 5.3300 - val_loss: 7.1406 - val_mean_absolute_error: 7.1406\n",
      "Epoch 466/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0457 - mean_absolute_error: 4.0457 - val_loss: 7.6852 - val_mean_absolute_error: 7.6852\n",
      "Epoch 467/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.9694 - mean_absolute_error: 3.9694 - val_loss: 7.6976 - val_mean_absolute_error: 7.6976\n",
      "Epoch 468/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.2166 - mean_absolute_error: 2.2166 - val_loss: 7.4881 - val_mean_absolute_error: 7.4881\n",
      "Epoch 469/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.6991 - mean_absolute_error: 2.6991 - val_loss: 7.2505 - val_mean_absolute_error: 7.2505\n",
      "Epoch 470/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 4.1270 - mean_absolute_error: 4.1270 - val_loss: 7.5599 - val_mean_absolute_error: 7.5599\n",
      "Epoch 471/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 10.4267 - mean_absolute_error: 10.4267 - val_loss: 6.6396 - val_mean_absolute_error: 6.6396\n",
      "Epoch 472/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 4.4291 - mean_absolute_error: 4.4291 - val_loss: 7.2956 - val_mean_absolute_error: 7.2956\n",
      "Epoch 473/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.8029 - mean_absolute_error: 3.8029 - val_loss: 7.4457 - val_mean_absolute_error: 7.4457\n",
      "Epoch 474/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.1069 - mean_absolute_error: 3.1069 - val_loss: 7.4445 - val_mean_absolute_error: 7.4445\n",
      "Epoch 475/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.6597 - mean_absolute_error: 2.6597 - val_loss: 7.4582 - val_mean_absolute_error: 7.4582\n",
      "Epoch 476/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 5.0485 - mean_absolute_error: 5.0485 - val_loss: 8.4301 - val_mean_absolute_error: 8.4301\n",
      "Epoch 477/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.3427 - mean_absolute_error: 3.3427 - val_loss: 7.3641 - val_mean_absolute_error: 7.3641\n",
      "Epoch 478/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.5329 - mean_absolute_error: 4.5329 - val_loss: 7.0542 - val_mean_absolute_error: 7.0543\n",
      "Epoch 479/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.2009 - mean_absolute_error: 3.2009 - val_loss: 7.3216 - val_mean_absolute_error: 7.3216\n",
      "Epoch 480/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.9872 - mean_absolute_error: 4.9872 - val_loss: 7.2095 - val_mean_absolute_error: 7.2095\n",
      "Epoch 481/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 5.1826 - mean_absolute_error: 5.1826 - val_loss: 7.1785 - val_mean_absolute_error: 7.1785\n",
      "Epoch 482/800\n",
      "1164/1164 [==============================] - 0s 133us/sample - loss: 3.3629 - mean_absolute_error: 3.3629 - val_loss: 7.6633 - val_mean_absolute_error: 7.6633\n",
      "Epoch 483/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 9.4429 - mean_absolute_error: 9.4429 - val_loss: 6.8944 - val_mean_absolute_error: 6.8944\n",
      "Epoch 484/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 6.7708 - mean_absolute_error: 6.7708 - val_loss: 7.4533 - val_mean_absolute_error: 7.4533\n",
      "Epoch 485/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 2.9888 - mean_absolute_error: 2.9888 - val_loss: 7.1041 - val_mean_absolute_error: 7.1041\n",
      "Epoch 486/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 3.3737 - mean_absolute_error: 3.3737 - val_loss: 7.2284 - val_mean_absolute_error: 7.2284\n",
      "Epoch 487/800\n",
      "1164/1164 [==============================] - 0s 133us/sample - loss: 3.8020 - mean_absolute_error: 3.8020 - val_loss: 7.3129 - val_mean_absolute_error: 7.3129\n",
      "Epoch 488/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.6672 - mean_absolute_error: 2.6672 - val_loss: 7.4626 - val_mean_absolute_error: 7.4626\n",
      "Epoch 489/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.4345 - mean_absolute_error: 2.4345 - val_loss: 7.2290 - val_mean_absolute_error: 7.2290\n",
      "Epoch 490/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.2671 - mean_absolute_error: 3.2671 - val_loss: 7.5099 - val_mean_absolute_error: 7.5099\n",
      "Epoch 491/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.5534 - mean_absolute_error: 2.5534 - val_loss: 7.2930 - val_mean_absolute_error: 7.2930\n",
      "Epoch 492/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 3.9139 - mean_absolute_error: 3.9139 - val_loss: 7.6456 - val_mean_absolute_error: 7.6456\n",
      "Epoch 493/800\n",
      "1164/1164 [==============================] - 0s 133us/sample - loss: 3.9091 - mean_absolute_error: 3.9091 - val_loss: 7.5324 - val_mean_absolute_error: 7.5324\n",
      "Epoch 494/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 1.5782 - mean_absolute_error: 1.5782 - val_loss: 7.1694 - val_mean_absolute_error: 7.1694\n",
      "Epoch 495/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5777 - mean_absolute_error: 3.5777 - val_loss: 7.2753 - val_mean_absolute_error: 7.2753\n",
      "Epoch 496/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.7172 - mean_absolute_error: 2.7172 - val_loss: 7.4713 - val_mean_absolute_error: 7.4713\n",
      "Epoch 497/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 2.8035 - mean_absolute_error: 2.8035 - val_loss: 7.4627 - val_mean_absolute_error: 7.4627\n",
      "Epoch 498/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.1326 - mean_absolute_error: 3.1326 - val_loss: 7.3715 - val_mean_absolute_error: 7.3715\n",
      "Epoch 499/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.5712 - mean_absolute_error: 2.5712 - val_loss: 7.2126 - val_mean_absolute_error: 7.2126\n",
      "Epoch 500/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.5365 - mean_absolute_error: 2.5365 - val_loss: 7.6783 - val_mean_absolute_error: 7.6783\n",
      "Epoch 501/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.6205 - mean_absolute_error: 4.6205 - val_loss: 7.6975 - val_mean_absolute_error: 7.6975\n",
      "Epoch 502/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.2864 - mean_absolute_error: 3.2864 - val_loss: 7.3573 - val_mean_absolute_error: 7.3573\n",
      "Epoch 503/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.7717 - mean_absolute_error: 2.7717 - val_loss: 7.4053 - val_mean_absolute_error: 7.4053\n",
      "Epoch 504/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 1.9701 - mean_absolute_error: 1.9701 - val_loss: 7.3309 - val_mean_absolute_error: 7.3309\n",
      "Epoch 505/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.8865 - mean_absolute_error: 3.8865 - val_loss: 7.0171 - val_mean_absolute_error: 7.0171\n",
      "Epoch 506/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.2766 - mean_absolute_error: 3.2766 - val_loss: 7.5292 - val_mean_absolute_error: 7.5292\n",
      "Epoch 507/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 5.7574 - mean_absolute_error: 5.7574 - val_loss: 7.1537 - val_mean_absolute_error: 7.1537\n",
      "Epoch 508/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.5036 - mean_absolute_error: 3.5036 - val_loss: 7.2711 - val_mean_absolute_error: 7.2711\n",
      "Epoch 509/800\n",
      "1164/1164 [==============================] - 0s 157us/sample - loss: 2.8424 - mean_absolute_error: 2.8424 - val_loss: 7.1133 - val_mean_absolute_error: 7.1133\n",
      "Epoch 510/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.8591 - mean_absolute_error: 2.8591 - val_loss: 7.5275 - val_mean_absolute_error: 7.5275\n",
      "Epoch 511/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 2.4021 - mean_absolute_error: 2.4021 - val_loss: 7.5489 - val_mean_absolute_error: 7.5489\n",
      "Epoch 512/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 2.5213 - mean_absolute_error: 2.5213 - val_loss: 7.4322 - val_mean_absolute_error: 7.4322\n",
      "Epoch 513/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 6.2706 - mean_absolute_error: 6.2706 - val_loss: 7.7195 - val_mean_absolute_error: 7.7195\n",
      "Epoch 514/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 8.1454 - mean_absolute_error: 8.1454 - val_loss: 7.2375 - val_mean_absolute_error: 7.2375\n",
      "Epoch 515/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.4636 - mean_absolute_error: 3.4636 - val_loss: 7.5699 - val_mean_absolute_error: 7.5699\n",
      "Epoch 516/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.0895 - mean_absolute_error: 3.0895 - val_loss: 7.1407 - val_mean_absolute_error: 7.1407\n",
      "Epoch 517/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.7364 - mean_absolute_error: 2.7364 - val_loss: 7.4952 - val_mean_absolute_error: 7.4952\n",
      "Epoch 518/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.4277 - mean_absolute_error: 2.4277 - val_loss: 7.1111 - val_mean_absolute_error: 7.1111\n",
      "Epoch 519/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.7052 - mean_absolute_error: 3.7052 - val_loss: 7.0557 - val_mean_absolute_error: 7.0557\n",
      "Epoch 520/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.9425 - mean_absolute_error: 1.9425 - val_loss: 7.4194 - val_mean_absolute_error: 7.4194\n",
      "Epoch 521/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.7399 - mean_absolute_error: 2.7399 - val_loss: 8.0218 - val_mean_absolute_error: 8.0218\n",
      "Epoch 522/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.2491 - mean_absolute_error: 3.2491 - val_loss: 7.8176 - val_mean_absolute_error: 7.8176\n",
      "Epoch 523/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.7337 - mean_absolute_error: 3.7337 - val_loss: 7.6045 - val_mean_absolute_error: 7.6045\n",
      "Epoch 524/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.1435 - mean_absolute_error: 2.1435 - val_loss: 7.3854 - val_mean_absolute_error: 7.3854\n",
      "Epoch 525/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.9312 - mean_absolute_error: 2.9312 - val_loss: 7.3943 - val_mean_absolute_error: 7.3943\n",
      "Epoch 526/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.6738 - mean_absolute_error: 3.6738 - val_loss: 7.2810 - val_mean_absolute_error: 7.2810\n",
      "Epoch 527/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 2.1292 - mean_absolute_error: 2.1292 - val_loss: 7.6009 - val_mean_absolute_error: 7.6009\n",
      "Epoch 528/800\n",
      "1164/1164 [==============================] - 0s 133us/sample - loss: 3.3942 - mean_absolute_error: 3.3942 - val_loss: 7.9235 - val_mean_absolute_error: 7.9235\n",
      "Epoch 529/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.5249 - mean_absolute_error: 5.5249 - val_loss: 7.6330 - val_mean_absolute_error: 7.6330\n",
      "Epoch 530/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.5540 - mean_absolute_error: 3.5540 - val_loss: 7.2695 - val_mean_absolute_error: 7.2695\n",
      "Epoch 531/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 2.4994 - mean_absolute_error: 2.4994 - val_loss: 7.2104 - val_mean_absolute_error: 7.2104\n",
      "Epoch 532/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 2.5649 - mean_absolute_error: 2.5649 - val_loss: 7.3891 - val_mean_absolute_error: 7.3891\n",
      "Epoch 533/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.7157 - mean_absolute_error: 1.7157 - val_loss: 7.6020 - val_mean_absolute_error: 7.6020\n",
      "Epoch 534/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.0281 - mean_absolute_error: 4.0281 - val_loss: 7.5078 - val_mean_absolute_error: 7.5078\n",
      "Epoch 535/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.9941 - mean_absolute_error: 4.9941 - val_loss: 7.0568 - val_mean_absolute_error: 7.0568\n",
      "Epoch 536/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.4631 - mean_absolute_error: 3.4631 - val_loss: 7.3752 - val_mean_absolute_error: 7.3752\n",
      "Epoch 537/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.9151 - mean_absolute_error: 2.9151 - val_loss: 7.2243 - val_mean_absolute_error: 7.2243\n",
      "Epoch 538/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.6025 - mean_absolute_error: 3.6025 - val_loss: 7.3543 - val_mean_absolute_error: 7.3543\n",
      "Epoch 539/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.8142 - mean_absolute_error: 2.8142 - val_loss: 7.0032 - val_mean_absolute_error: 7.0032\n",
      "Epoch 540/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.6980 - mean_absolute_error: 4.6980 - val_loss: 7.7248 - val_mean_absolute_error: 7.7248\n",
      "Epoch 541/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.8990 - mean_absolute_error: 4.8990 - val_loss: 7.2560 - val_mean_absolute_error: 7.2560\n",
      "Epoch 542/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 1.9897 - mean_absolute_error: 1.9897 - val_loss: 7.3507 - val_mean_absolute_error: 7.3507\n",
      "Epoch 543/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.3587 - mean_absolute_error: 3.3587 - val_loss: 7.5008 - val_mean_absolute_error: 7.5008\n",
      "Epoch 544/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.2626 - mean_absolute_error: 2.2626 - val_loss: 7.2443 - val_mean_absolute_error: 7.2443\n",
      "Epoch 545/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.1671 - mean_absolute_error: 2.1671 - val_loss: 7.3545 - val_mean_absolute_error: 7.3545\n",
      "Epoch 546/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.3526 - mean_absolute_error: 2.3526 - val_loss: 7.4630 - val_mean_absolute_error: 7.4630\n",
      "Epoch 547/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.9065 - mean_absolute_error: 2.9065 - val_loss: 7.2775 - val_mean_absolute_error: 7.2775\n",
      "Epoch 548/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 7.5472 - mean_absolute_error: 7.5472 - val_loss: 7.3453 - val_mean_absolute_error: 7.3453\n",
      "Epoch 549/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 5.3855 - mean_absolute_error: 5.3855 - val_loss: 7.4644 - val_mean_absolute_error: 7.4644\n",
      "Epoch 550/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.5706 - mean_absolute_error: 2.5706 - val_loss: 7.0498 - val_mean_absolute_error: 7.0498\n",
      "Epoch 551/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.6527 - mean_absolute_error: 3.6527 - val_loss: 7.5357 - val_mean_absolute_error: 7.5357\n",
      "Epoch 552/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 4.6284 - mean_absolute_error: 4.6284 - val_loss: 7.5150 - val_mean_absolute_error: 7.5150\n",
      "Epoch 553/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 1.8448 - mean_absolute_error: 1.8448 - val_loss: 7.4740 - val_mean_absolute_error: 7.4740\n",
      "Epoch 554/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.5403 - mean_absolute_error: 2.5403 - val_loss: 7.3060 - val_mean_absolute_error: 7.3060\n",
      "Epoch 555/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.5633 - mean_absolute_error: 3.5633 - val_loss: 7.5017 - val_mean_absolute_error: 7.5017\n",
      "Epoch 556/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 3.1269 - mean_absolute_error: 3.1269 - val_loss: 7.4842 - val_mean_absolute_error: 7.4842\n",
      "Epoch 557/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.4416 - mean_absolute_error: 2.4416 - val_loss: 7.2504 - val_mean_absolute_error: 7.2504\n",
      "Epoch 558/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.9966 - mean_absolute_error: 1.9966 - val_loss: 7.4859 - val_mean_absolute_error: 7.4859\n",
      "Epoch 559/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.4935 - mean_absolute_error: 2.4935 - val_loss: 7.3623 - val_mean_absolute_error: 7.3623\n",
      "Epoch 560/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 1.5592 - mean_absolute_error: 1.5592 - val_loss: 7.2990 - val_mean_absolute_error: 7.2990\n",
      "Epoch 561/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.3166 - mean_absolute_error: 2.3166 - val_loss: 7.3611 - val_mean_absolute_error: 7.3611\n",
      "Epoch 562/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.9810 - mean_absolute_error: 2.9810 - val_loss: 7.5509 - val_mean_absolute_error: 7.5509\n",
      "Epoch 563/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.5781 - mean_absolute_error: 2.5781 - val_loss: 7.2079 - val_mean_absolute_error: 7.2079\n",
      "Epoch 564/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 6.1718 - mean_absolute_error: 6.1718 - val_loss: 6.7834 - val_mean_absolute_error: 6.7834\n",
      "Epoch 565/800\n",
      "1164/1164 [==============================] - 0s 155us/sample - loss: 7.6668 - mean_absolute_error: 7.6668 - val_loss: 6.9095 - val_mean_absolute_error: 6.9095\n",
      "Epoch 566/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.8508 - mean_absolute_error: 2.8508 - val_loss: 7.5488 - val_mean_absolute_error: 7.5488\n",
      "Epoch 567/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.8230 - mean_absolute_error: 2.8230 - val_loss: 7.4395 - val_mean_absolute_error: 7.4395\n",
      "Epoch 568/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.3778 - mean_absolute_error: 3.3778 - val_loss: 7.3459 - val_mean_absolute_error: 7.3459\n",
      "Epoch 569/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 4.0334 - mean_absolute_error: 4.0334 - val_loss: 7.2572 - val_mean_absolute_error: 7.2572\n",
      "Epoch 570/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.8841 - mean_absolute_error: 3.8841 - val_loss: 7.4634 - val_mean_absolute_error: 7.4634\n",
      "Epoch 571/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.4551 - mean_absolute_error: 2.4551 - val_loss: 7.7088 - val_mean_absolute_error: 7.7088\n",
      "Epoch 572/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.1583 - mean_absolute_error: 3.1583 - val_loss: 7.8054 - val_mean_absolute_error: 7.8054\n",
      "Epoch 573/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.3259 - mean_absolute_error: 3.3259 - val_loss: 7.9438 - val_mean_absolute_error: 7.9438\n",
      "Epoch 574/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.8375 - mean_absolute_error: 2.8375 - val_loss: 7.6132 - val_mean_absolute_error: 7.6132\n",
      "Epoch 575/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.7570 - mean_absolute_error: 1.7570 - val_loss: 7.7361 - val_mean_absolute_error: 7.7361\n",
      "Epoch 576/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.4484 - mean_absolute_error: 2.4484 - val_loss: 7.7717 - val_mean_absolute_error: 7.7717\n",
      "Epoch 577/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 4.0351 - mean_absolute_error: 4.0351 - val_loss: 7.8133 - val_mean_absolute_error: 7.8133\n",
      "Epoch 578/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.2308 - mean_absolute_error: 3.2308 - val_loss: 7.5880 - val_mean_absolute_error: 7.5880\n",
      "Epoch 579/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 4.1052 - mean_absolute_error: 4.1052 - val_loss: 7.2566 - val_mean_absolute_error: 7.2566\n",
      "Epoch 580/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 1.5681 - mean_absolute_error: 1.5681 - val_loss: 7.4226 - val_mean_absolute_error: 7.4226\n",
      "Epoch 581/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.1743 - mean_absolute_error: 2.1743 - val_loss: 7.1816 - val_mean_absolute_error: 7.1816\n",
      "Epoch 582/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.7864 - mean_absolute_error: 2.7864 - val_loss: 7.5587 - val_mean_absolute_error: 7.5587\n",
      "Epoch 583/800\n",
      "1164/1164 [==============================] - 0s 166us/sample - loss: 3.0046 - mean_absolute_error: 3.0046 - val_loss: 7.5197 - val_mean_absolute_error: 7.5197\n",
      "Epoch 584/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.2790 - mean_absolute_error: 2.2790 - val_loss: 7.1990 - val_mean_absolute_error: 7.1990\n",
      "Epoch 585/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5195 - mean_absolute_error: 3.5195 - val_loss: 7.6236 - val_mean_absolute_error: 7.6236\n",
      "Epoch 586/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.7655 - mean_absolute_error: 2.7655 - val_loss: 7.7293 - val_mean_absolute_error: 7.7293\n",
      "Epoch 587/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 5.5017 - mean_absolute_error: 5.5017 - val_loss: 7.0823 - val_mean_absolute_error: 7.0823\n",
      "Epoch 588/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.2715 - mean_absolute_error: 3.2715 - val_loss: 8.0310 - val_mean_absolute_error: 8.0310\n",
      "Epoch 589/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.9853 - mean_absolute_error: 2.9853 - val_loss: 7.8282 - val_mean_absolute_error: 7.8282\n",
      "Epoch 590/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.8588 - mean_absolute_error: 3.8588 - val_loss: 7.3916 - val_mean_absolute_error: 7.3916\n",
      "Epoch 591/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.9762 - mean_absolute_error: 2.9762 - val_loss: 7.0964 - val_mean_absolute_error: 7.0964\n",
      "Epoch 592/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.8984 - mean_absolute_error: 2.8984 - val_loss: 7.6676 - val_mean_absolute_error: 7.6676\n",
      "Epoch 593/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 1.7064 - mean_absolute_error: 1.7064 - val_loss: 7.4291 - val_mean_absolute_error: 7.4291\n",
      "Epoch 594/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.8110 - mean_absolute_error: 1.8110 - val_loss: 7.4281 - val_mean_absolute_error: 7.4281\n",
      "Epoch 595/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.6008 - mean_absolute_error: 2.6008 - val_loss: 7.2608 - val_mean_absolute_error: 7.2608\n",
      "Epoch 596/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.5079 - mean_absolute_error: 4.5079 - val_loss: 7.5856 - val_mean_absolute_error: 7.5856\n",
      "Epoch 597/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.3026 - mean_absolute_error: 3.3026 - val_loss: 7.6341 - val_mean_absolute_error: 7.6341\n",
      "Epoch 598/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.3479 - mean_absolute_error: 2.3479 - val_loss: 7.7786 - val_mean_absolute_error: 7.7786\n",
      "Epoch 599/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.8024 - mean_absolute_error: 2.8024 - val_loss: 7.4134 - val_mean_absolute_error: 7.4134\n",
      "Epoch 600/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 3.1705 - mean_absolute_error: 3.1705 - val_loss: 7.6895 - val_mean_absolute_error: 7.6895\n",
      "Epoch 601/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 1.8747 - mean_absolute_error: 1.8747 - val_loss: 7.6208 - val_mean_absolute_error: 7.6208\n",
      "Epoch 602/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.5636 - mean_absolute_error: 1.5636 - val_loss: 7.5984 - val_mean_absolute_error: 7.5984\n",
      "Epoch 603/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 3.2802 - mean_absolute_error: 3.2802 - val_loss: 7.2435 - val_mean_absolute_error: 7.2435\n",
      "Epoch 604/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 3.6006 - mean_absolute_error: 3.6006 - val_loss: 7.3254 - val_mean_absolute_error: 7.3254\n",
      "Epoch 605/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.3358 - mean_absolute_error: 3.3358 - val_loss: 7.6723 - val_mean_absolute_error: 7.6723\n",
      "Epoch 606/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 4.5882 - mean_absolute_error: 4.5882 - val_loss: 7.1481 - val_mean_absolute_error: 7.1481\n",
      "Epoch 607/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 5.1033 - mean_absolute_error: 5.1033 - val_loss: 8.0143 - val_mean_absolute_error: 8.0143\n",
      "Epoch 608/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.0934 - mean_absolute_error: 3.0934 - val_loss: 7.5311 - val_mean_absolute_error: 7.5311\n",
      "Epoch 609/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.2551 - mean_absolute_error: 2.2551 - val_loss: 7.3314 - val_mean_absolute_error: 7.3314\n",
      "Epoch 610/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 3.5794 - mean_absolute_error: 3.5794 - val_loss: 7.6149 - val_mean_absolute_error: 7.6149\n",
      "Epoch 611/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.5889 - mean_absolute_error: 1.5889 - val_loss: 7.8337 - val_mean_absolute_error: 7.8337\n",
      "Epoch 612/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.6216 - mean_absolute_error: 4.6216 - val_loss: 7.5629 - val_mean_absolute_error: 7.5629\n",
      "Epoch 613/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.3174 - mean_absolute_error: 3.3174 - val_loss: 7.2657 - val_mean_absolute_error: 7.2657\n",
      "Epoch 614/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.5287 - mean_absolute_error: 2.5287 - val_loss: 7.7005 - val_mean_absolute_error: 7.7005\n",
      "Epoch 615/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.6082 - mean_absolute_error: 2.6082 - val_loss: 7.6634 - val_mean_absolute_error: 7.6634\n",
      "Epoch 616/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 1.6822 - mean_absolute_error: 1.6822 - val_loss: 7.3490 - val_mean_absolute_error: 7.3490\n",
      "Epoch 617/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.1454 - mean_absolute_error: 2.1454 - val_loss: 7.5458 - val_mean_absolute_error: 7.5458\n",
      "Epoch 618/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 4.1560 - mean_absolute_error: 4.1560 - val_loss: 7.5381 - val_mean_absolute_error: 7.5381\n",
      "Epoch 619/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.2796 - mean_absolute_error: 4.2796 - val_loss: 8.2725 - val_mean_absolute_error: 8.2725\n",
      "Epoch 620/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 3.6748 - mean_absolute_error: 3.6748 - val_loss: 7.6194 - val_mean_absolute_error: 7.6194\n",
      "Epoch 621/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.8241 - mean_absolute_error: 1.8241 - val_loss: 7.5736 - val_mean_absolute_error: 7.5736\n",
      "Epoch 622/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.4574 - mean_absolute_error: 2.4574 - val_loss: 7.4866 - val_mean_absolute_error: 7.4866\n",
      "Epoch 623/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 3.5128 - mean_absolute_error: 3.5128 - val_loss: 7.2892 - val_mean_absolute_error: 7.2892\n",
      "Epoch 624/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.2942 - mean_absolute_error: 3.2942 - val_loss: 7.4990 - val_mean_absolute_error: 7.4990\n",
      "Epoch 625/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.4284 - mean_absolute_error: 2.4284 - val_loss: 7.1353 - val_mean_absolute_error: 7.1353\n",
      "Epoch 626/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.0816 - mean_absolute_error: 3.0816 - val_loss: 7.2788 - val_mean_absolute_error: 7.2788\n",
      "Epoch 627/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.1017 - mean_absolute_error: 2.1017 - val_loss: 7.5813 - val_mean_absolute_error: 7.5813\n",
      "Epoch 628/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 2.9246 - mean_absolute_error: 2.9246 - val_loss: 7.6662 - val_mean_absolute_error: 7.6662\n",
      "Epoch 629/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.1646 - mean_absolute_error: 4.1646 - val_loss: 7.4170 - val_mean_absolute_error: 7.4170\n",
      "Epoch 630/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.1729 - mean_absolute_error: 2.1729 - val_loss: 7.3187 - val_mean_absolute_error: 7.3187\n",
      "Epoch 631/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 1.6632 - mean_absolute_error: 1.6632 - val_loss: 7.4968 - val_mean_absolute_error: 7.4968\n",
      "Epoch 632/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.3594 - mean_absolute_error: 2.3594 - val_loss: 7.3000 - val_mean_absolute_error: 7.3000\n",
      "Epoch 633/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 1.7451 - mean_absolute_error: 1.7451 - val_loss: 7.5808 - val_mean_absolute_error: 7.5808\n",
      "Epoch 634/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 1.6539 - mean_absolute_error: 1.6539 - val_loss: 7.6992 - val_mean_absolute_error: 7.6992\n",
      "Epoch 635/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.3478 - mean_absolute_error: 2.3478 - val_loss: 7.6217 - val_mean_absolute_error: 7.6217\n",
      "Epoch 636/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.5855 - mean_absolute_error: 3.5855 - val_loss: 7.6135 - val_mean_absolute_error: 7.6135\n",
      "Epoch 637/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.4843 - mean_absolute_error: 2.4843 - val_loss: 7.5112 - val_mean_absolute_error: 7.5112\n",
      "Epoch 638/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.5503 - mean_absolute_error: 2.5503 - val_loss: 7.0057 - val_mean_absolute_error: 7.0057\n",
      "Epoch 639/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.8795 - mean_absolute_error: 3.8795 - val_loss: 8.1605 - val_mean_absolute_error: 8.1605\n",
      "Epoch 640/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 4.3754 - mean_absolute_error: 4.3754 - val_loss: 7.9017 - val_mean_absolute_error: 7.9017\n",
      "Epoch 641/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 4.4020 - mean_absolute_error: 4.4020 - val_loss: 7.7296 - val_mean_absolute_error: 7.7296\n",
      "Epoch 642/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 4.0276 - mean_absolute_error: 4.0276 - val_loss: 7.3964 - val_mean_absolute_error: 7.3964\n",
      "Epoch 643/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.8926 - mean_absolute_error: 3.8926 - val_loss: 7.4172 - val_mean_absolute_error: 7.4172\n",
      "Epoch 644/800\n",
      "1164/1164 [==============================] - 0s 153us/sample - loss: 5.1782 - mean_absolute_error: 5.1782 - val_loss: 7.1789 - val_mean_absolute_error: 7.1789\n",
      "Epoch 645/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.3624 - mean_absolute_error: 2.3624 - val_loss: 7.5162 - val_mean_absolute_error: 7.5162\n",
      "Epoch 646/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.5811 - mean_absolute_error: 2.5811 - val_loss: 7.3432 - val_mean_absolute_error: 7.3432\n",
      "Epoch 647/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.0332 - mean_absolute_error: 2.0332 - val_loss: 7.5617 - val_mean_absolute_error: 7.5617\n",
      "Epoch 648/800\n",
      "1164/1164 [==============================] - 0s 158us/sample - loss: 3.5663 - mean_absolute_error: 3.5663 - val_loss: 7.8259 - val_mean_absolute_error: 7.8259\n",
      "Epoch 649/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.9434 - mean_absolute_error: 2.9434 - val_loss: 7.3270 - val_mean_absolute_error: 7.3270\n",
      "Epoch 650/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.5287 - mean_absolute_error: 2.5287 - val_loss: 7.8123 - val_mean_absolute_error: 7.8123\n",
      "Epoch 651/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.5443 - mean_absolute_error: 3.5443 - val_loss: 7.0026 - val_mean_absolute_error: 7.0026\n",
      "Epoch 652/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 4.3894 - mean_absolute_error: 4.3894 - val_loss: 7.4130 - val_mean_absolute_error: 7.4130\n",
      "Epoch 653/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 1.9772 - mean_absolute_error: 1.9772 - val_loss: 7.4373 - val_mean_absolute_error: 7.4373\n",
      "Epoch 654/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 4.2570 - mean_absolute_error: 4.2570 - val_loss: 7.7707 - val_mean_absolute_error: 7.7707\n",
      "Epoch 655/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 4.8608 - mean_absolute_error: 4.8608 - val_loss: 7.6314 - val_mean_absolute_error: 7.6314\n",
      "Epoch 656/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.5675 - mean_absolute_error: 2.5675 - val_loss: 7.7833 - val_mean_absolute_error: 7.7833\n",
      "Epoch 657/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 5.8471 - mean_absolute_error: 5.8471 - val_loss: 7.4941 - val_mean_absolute_error: 7.4941\n",
      "Epoch 658/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 7.0748 - mean_absolute_error: 7.0748 - val_loss: 7.6154 - val_mean_absolute_error: 7.6154\n",
      "Epoch 659/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 5.8284 - mean_absolute_error: 5.8284 - val_loss: 7.1919 - val_mean_absolute_error: 7.1919\n",
      "Epoch 660/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.7232 - mean_absolute_error: 3.7232 - val_loss: 7.5995 - val_mean_absolute_error: 7.5995\n",
      "Epoch 661/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.7343 - mean_absolute_error: 2.7343 - val_loss: 7.1688 - val_mean_absolute_error: 7.1688\n",
      "Epoch 662/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.5449 - mean_absolute_error: 3.5449 - val_loss: 7.2535 - val_mean_absolute_error: 7.2535\n",
      "Epoch 663/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.5445 - mean_absolute_error: 2.5445 - val_loss: 8.0499 - val_mean_absolute_error: 8.0499\n",
      "Epoch 664/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.8905 - mean_absolute_error: 4.8905 - val_loss: 7.3035 - val_mean_absolute_error: 7.3035\n",
      "Epoch 665/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.9266 - mean_absolute_error: 2.9266 - val_loss: 7.7001 - val_mean_absolute_error: 7.7001\n",
      "Epoch 666/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 3.3699 - mean_absolute_error: 3.3699 - val_loss: 7.5067 - val_mean_absolute_error: 7.5067\n",
      "Epoch 667/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.3207 - mean_absolute_error: 3.3207 - val_loss: 7.3200 - val_mean_absolute_error: 7.3200\n",
      "Epoch 668/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.7570 - mean_absolute_error: 1.7570 - val_loss: 7.4701 - val_mean_absolute_error: 7.4701\n",
      "Epoch 669/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 2.7584 - mean_absolute_error: 2.7584 - val_loss: 7.5024 - val_mean_absolute_error: 7.5024\n",
      "Epoch 670/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.3264 - mean_absolute_error: 3.3264 - val_loss: 8.0414 - val_mean_absolute_error: 8.0414\n",
      "Epoch 671/800\n",
      "1164/1164 [==============================] - 0s 133us/sample - loss: 2.7442 - mean_absolute_error: 2.7442 - val_loss: 7.7144 - val_mean_absolute_error: 7.7144\n",
      "Epoch 672/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.8580 - mean_absolute_error: 1.8580 - val_loss: 7.4080 - val_mean_absolute_error: 7.4080\n",
      "Epoch 673/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.1551 - mean_absolute_error: 2.1551 - val_loss: 7.4149 - val_mean_absolute_error: 7.4149\n",
      "Epoch 674/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.2655 - mean_absolute_error: 2.2655 - val_loss: 8.0004 - val_mean_absolute_error: 8.0004\n",
      "Epoch 675/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 4.0350 - mean_absolute_error: 4.0350 - val_loss: 7.2814 - val_mean_absolute_error: 7.2814\n",
      "Epoch 676/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 2.1267 - mean_absolute_error: 2.1267 - val_loss: 7.5169 - val_mean_absolute_error: 7.5169\n",
      "Epoch 677/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 1.6522 - mean_absolute_error: 1.6522 - val_loss: 7.3949 - val_mean_absolute_error: 7.3949\n",
      "Epoch 678/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.7386 - mean_absolute_error: 1.7386 - val_loss: 7.5731 - val_mean_absolute_error: 7.5731\n",
      "Epoch 679/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 3.7630 - mean_absolute_error: 3.7630 - val_loss: 7.4397 - val_mean_absolute_error: 7.4397\n",
      "Epoch 680/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 6.3172 - mean_absolute_error: 6.3172 - val_loss: 7.6026 - val_mean_absolute_error: 7.6026\n",
      "Epoch 681/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.1854 - mean_absolute_error: 3.1854 - val_loss: 7.5259 - val_mean_absolute_error: 7.5259\n",
      "Epoch 682/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 2.3515 - mean_absolute_error: 2.3515 - val_loss: 7.6760 - val_mean_absolute_error: 7.6760\n",
      "Epoch 683/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 2.7792 - mean_absolute_error: 2.7792 - val_loss: 7.5594 - val_mean_absolute_error: 7.5594\n",
      "Epoch 684/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.1959 - mean_absolute_error: 3.1959 - val_loss: 7.7307 - val_mean_absolute_error: 7.7307\n",
      "Epoch 685/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.5521 - mean_absolute_error: 2.5521 - val_loss: 7.4829 - val_mean_absolute_error: 7.4829\n",
      "Epoch 686/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.2230 - mean_absolute_error: 4.2230 - val_loss: 7.7712 - val_mean_absolute_error: 7.7712\n",
      "Epoch 687/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.3682 - mean_absolute_error: 3.3682 - val_loss: 7.8081 - val_mean_absolute_error: 7.8081\n",
      "Epoch 688/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 3.0313 - mean_absolute_error: 3.0313 - val_loss: 7.6415 - val_mean_absolute_error: 7.6415\n",
      "Epoch 689/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.4591 - mean_absolute_error: 2.4591 - val_loss: 7.5380 - val_mean_absolute_error: 7.5380\n",
      "Epoch 690/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.0968 - mean_absolute_error: 2.0968 - val_loss: 7.5109 - val_mean_absolute_error: 7.5109\n",
      "Epoch 691/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.6605 - mean_absolute_error: 2.6605 - val_loss: 7.5137 - val_mean_absolute_error: 7.5137\n",
      "Epoch 692/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.6893 - mean_absolute_error: 2.6893 - val_loss: 8.0619 - val_mean_absolute_error: 8.0619\n",
      "Epoch 693/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.9024 - mean_absolute_error: 3.9024 - val_loss: 7.2189 - val_mean_absolute_error: 7.2189\n",
      "Epoch 694/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.9638 - mean_absolute_error: 2.9638 - val_loss: 7.6484 - val_mean_absolute_error: 7.6484\n",
      "Epoch 695/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 7.1209 - mean_absolute_error: 7.1209 - val_loss: 7.3775 - val_mean_absolute_error: 7.3775\n",
      "Epoch 696/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.4504 - mean_absolute_error: 2.4504 - val_loss: 7.4535 - val_mean_absolute_error: 7.4535\n",
      "Epoch 697/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.0271 - mean_absolute_error: 2.0271 - val_loss: 7.8417 - val_mean_absolute_error: 7.8417\n",
      "Epoch 698/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.1878 - mean_absolute_error: 2.1878 - val_loss: 7.7939 - val_mean_absolute_error: 7.7939\n",
      "Epoch 699/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.2015 - mean_absolute_error: 3.2015 - val_loss: 7.5557 - val_mean_absolute_error: 7.5557\n",
      "Epoch 700/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.0233 - mean_absolute_error: 3.0233 - val_loss: 7.4844 - val_mean_absolute_error: 7.4844\n",
      "Epoch 701/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.0609 - mean_absolute_error: 2.0609 - val_loss: 7.4323 - val_mean_absolute_error: 7.4323\n",
      "Epoch 702/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.8813 - mean_absolute_error: 2.8813 - val_loss: 7.4735 - val_mean_absolute_error: 7.4735\n",
      "Epoch 703/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.5590 - mean_absolute_error: 2.5590 - val_loss: 7.8044 - val_mean_absolute_error: 7.8044\n",
      "Epoch 704/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 4.2116 - mean_absolute_error: 4.2116 - val_loss: 7.9239 - val_mean_absolute_error: 7.9239\n",
      "Epoch 705/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.8254 - mean_absolute_error: 4.8254 - val_loss: 7.2786 - val_mean_absolute_error: 7.2786\n",
      "Epoch 706/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 3.6241 - mean_absolute_error: 3.6241 - val_loss: 7.7899 - val_mean_absolute_error: 7.7899\n",
      "Epoch 707/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 1.6888 - mean_absolute_error: 1.6888 - val_loss: 7.9728 - val_mean_absolute_error: 7.9728\n",
      "Epoch 708/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.4627 - mean_absolute_error: 2.4627 - val_loss: 7.4614 - val_mean_absolute_error: 7.4614\n",
      "Epoch 709/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 3.2909 - mean_absolute_error: 3.2909 - val_loss: 7.6097 - val_mean_absolute_error: 7.6097\n",
      "Epoch 710/800\n",
      "1164/1164 [==============================] - 0s 154us/sample - loss: 2.1510 - mean_absolute_error: 2.1510 - val_loss: 8.0377 - val_mean_absolute_error: 8.0377\n",
      "Epoch 711/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.5383 - mean_absolute_error: 2.5383 - val_loss: 7.4696 - val_mean_absolute_error: 7.4696\n",
      "Epoch 712/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.7706 - mean_absolute_error: 2.7706 - val_loss: 7.6073 - val_mean_absolute_error: 7.6073\n",
      "Epoch 713/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 2.7060 - mean_absolute_error: 2.7060 - val_loss: 7.5801 - val_mean_absolute_error: 7.5801\n",
      "Epoch 714/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.2803 - mean_absolute_error: 3.2803 - val_loss: 7.5970 - val_mean_absolute_error: 7.5970\n",
      "Epoch 715/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 2.7781 - mean_absolute_error: 2.7781 - val_loss: 7.4643 - val_mean_absolute_error: 7.4643\n",
      "Epoch 716/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.0410 - mean_absolute_error: 2.0410 - val_loss: 7.5250 - val_mean_absolute_error: 7.5250\n",
      "Epoch 717/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.1062 - mean_absolute_error: 3.1062 - val_loss: 7.3897 - val_mean_absolute_error: 7.3897\n",
      "Epoch 718/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 1.2236 - mean_absolute_error: 1.2236 - val_loss: 7.6651 - val_mean_absolute_error: 7.6651\n",
      "Epoch 719/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 3.4149 - mean_absolute_error: 3.4149 - val_loss: 7.1240 - val_mean_absolute_error: 7.1240\n",
      "Epoch 720/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 11.5438 - mean_absolute_error: 11.5438 - val_loss: 7.6100 - val_mean_absolute_error: 7.6100\n",
      "Epoch 721/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 3.0931 - mean_absolute_error: 3.0931 - val_loss: 7.3223 - val_mean_absolute_error: 7.3223\n",
      "Epoch 722/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.9626 - mean_absolute_error: 1.9626 - val_loss: 7.4974 - val_mean_absolute_error: 7.4974\n",
      "Epoch 723/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.8859 - mean_absolute_error: 2.8859 - val_loss: 7.5235 - val_mean_absolute_error: 7.5235\n",
      "Epoch 724/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 3.1030 - mean_absolute_error: 3.1030 - val_loss: 7.6841 - val_mean_absolute_error: 7.6841\n",
      "Epoch 725/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.3966 - mean_absolute_error: 2.3966 - val_loss: 7.6429 - val_mean_absolute_error: 7.6429\n",
      "Epoch 726/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 4.4502 - mean_absolute_error: 4.4502 - val_loss: 8.0881 - val_mean_absolute_error: 8.0881\n",
      "Epoch 727/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.0703 - mean_absolute_error: 3.0703 - val_loss: 7.5555 - val_mean_absolute_error: 7.5555\n",
      "Epoch 728/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.0670 - mean_absolute_error: 2.0670 - val_loss: 7.2425 - val_mean_absolute_error: 7.2425\n",
      "Epoch 729/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.8854 - mean_absolute_error: 2.8854 - val_loss: 7.3421 - val_mean_absolute_error: 7.3421\n",
      "Epoch 730/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.3334 - mean_absolute_error: 2.3334 - val_loss: 7.4482 - val_mean_absolute_error: 7.4482\n",
      "Epoch 731/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 1.6638 - mean_absolute_error: 1.6638 - val_loss: 7.5780 - val_mean_absolute_error: 7.5780\n",
      "Epoch 732/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.7734 - mean_absolute_error: 3.7734 - val_loss: 7.4767 - val_mean_absolute_error: 7.4767\n",
      "Epoch 733/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.9313 - mean_absolute_error: 2.9313 - val_loss: 7.3636 - val_mean_absolute_error: 7.3636\n",
      "Epoch 734/800\n",
      "1164/1164 [==============================] - 0s 156us/sample - loss: 2.5366 - mean_absolute_error: 2.5366 - val_loss: 7.6678 - val_mean_absolute_error: 7.6678\n",
      "Epoch 735/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 2.7731 - mean_absolute_error: 2.7731 - val_loss: 7.5533 - val_mean_absolute_error: 7.5533\n",
      "Epoch 736/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 1.9688 - mean_absolute_error: 1.9688 - val_loss: 7.4924 - val_mean_absolute_error: 7.4924\n",
      "Epoch 737/800\n",
      "1164/1164 [==============================] - 0s 157us/sample - loss: 1.8883 - mean_absolute_error: 1.8883 - val_loss: 7.5161 - val_mean_absolute_error: 7.5161\n",
      "Epoch 738/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.0268 - mean_absolute_error: 3.0268 - val_loss: 7.1939 - val_mean_absolute_error: 7.1939\n",
      "Epoch 739/800\n",
      "1164/1164 [==============================] - 0s 136us/sample - loss: 2.2993 - mean_absolute_error: 2.2993 - val_loss: 7.2425 - val_mean_absolute_error: 7.2425\n",
      "Epoch 740/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.4193 - mean_absolute_error: 3.4193 - val_loss: 7.3814 - val_mean_absolute_error: 7.3814\n",
      "Epoch 741/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 3.7645 - mean_absolute_error: 3.7645 - val_loss: 7.2969 - val_mean_absolute_error: 7.2969\n",
      "Epoch 742/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.1518 - mean_absolute_error: 2.1518 - val_loss: 7.7336 - val_mean_absolute_error: 7.7336\n",
      "Epoch 743/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 1.9627 - mean_absolute_error: 1.9627 - val_loss: 7.6685 - val_mean_absolute_error: 7.6685\n",
      "Epoch 744/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.9123 - mean_absolute_error: 3.9123 - val_loss: 7.4273 - val_mean_absolute_error: 7.4273\n",
      "Epoch 745/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.5989 - mean_absolute_error: 2.5989 - val_loss: 7.4857 - val_mean_absolute_error: 7.4857\n",
      "Epoch 746/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.0798 - mean_absolute_error: 2.0798 - val_loss: 7.6471 - val_mean_absolute_error: 7.6471\n",
      "Epoch 747/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 2.2319 - mean_absolute_error: 2.2319 - val_loss: 7.3214 - val_mean_absolute_error: 7.3214\n",
      "Epoch 748/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 2.1751 - mean_absolute_error: 2.1751 - val_loss: 7.3560 - val_mean_absolute_error: 7.3560\n",
      "Epoch 749/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 6.9874 - mean_absolute_error: 6.9874 - val_loss: 7.5339 - val_mean_absolute_error: 7.5339\n",
      "Epoch 750/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 5.6869 - mean_absolute_error: 5.6869 - val_loss: 7.6628 - val_mean_absolute_error: 7.6628\n",
      "Epoch 751/800\n",
      "1164/1164 [==============================] - 0s 151us/sample - loss: 4.6627 - mean_absolute_error: 4.6627 - val_loss: 7.6195 - val_mean_absolute_error: 7.6195\n",
      "Epoch 752/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.8466 - mean_absolute_error: 2.8466 - val_loss: 7.5737 - val_mean_absolute_error: 7.5737\n",
      "Epoch 753/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.3213 - mean_absolute_error: 3.3213 - val_loss: 7.4969 - val_mean_absolute_error: 7.4969\n",
      "Epoch 754/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.4119 - mean_absolute_error: 2.4119 - val_loss: 7.6286 - val_mean_absolute_error: 7.6286\n",
      "Epoch 755/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 1.6529 - mean_absolute_error: 1.6529 - val_loss: 7.6234 - val_mean_absolute_error: 7.6234\n",
      "Epoch 756/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 3.6023 - mean_absolute_error: 3.6023 - val_loss: 7.4125 - val_mean_absolute_error: 7.4125\n",
      "Epoch 757/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 3.8865 - mean_absolute_error: 3.8865 - val_loss: 7.8788 - val_mean_absolute_error: 7.8788\n",
      "Epoch 758/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 1.9504 - mean_absolute_error: 1.9504 - val_loss: 7.8559 - val_mean_absolute_error: 7.8559\n",
      "Epoch 759/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.7112 - mean_absolute_error: 2.7112 - val_loss: 7.5261 - val_mean_absolute_error: 7.5261\n",
      "Epoch 760/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.3790 - mean_absolute_error: 2.3790 - val_loss: 7.8091 - val_mean_absolute_error: 7.8091\n",
      "Epoch 761/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 1.7094 - mean_absolute_error: 1.7095 - val_loss: 7.5555 - val_mean_absolute_error: 7.5555\n",
      "Epoch 762/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.4694 - mean_absolute_error: 2.4694 - val_loss: 7.5767 - val_mean_absolute_error: 7.5767\n",
      "Epoch 763/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 4.1583 - mean_absolute_error: 4.1583 - val_loss: 7.6175 - val_mean_absolute_error: 7.6175\n",
      "Epoch 764/800\n",
      "1164/1164 [==============================] - 0s 150us/sample - loss: 2.7326 - mean_absolute_error: 2.7326 - val_loss: 7.5643 - val_mean_absolute_error: 7.5643\n",
      "Epoch 765/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 1.5721 - mean_absolute_error: 1.5721 - val_loss: 7.4449 - val_mean_absolute_error: 7.4449\n",
      "Epoch 766/800\n",
      "1164/1164 [==============================] - 0s 137us/sample - loss: 1.8667 - mean_absolute_error: 1.8667 - val_loss: 7.3506 - val_mean_absolute_error: 7.3506\n",
      "Epoch 767/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.5357 - mean_absolute_error: 3.5357 - val_loss: 7.3330 - val_mean_absolute_error: 7.3330\n",
      "Epoch 768/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 3.3974 - mean_absolute_error: 3.3974 - val_loss: 7.5741 - val_mean_absolute_error: 7.5741\n",
      "Epoch 769/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 1.9877 - mean_absolute_error: 1.9877 - val_loss: 7.9564 - val_mean_absolute_error: 7.9564\n",
      "Epoch 770/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.6972 - mean_absolute_error: 2.6972 - val_loss: 7.5127 - val_mean_absolute_error: 7.5127\n",
      "Epoch 771/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.7806 - mean_absolute_error: 2.7806 - val_loss: 7.5943 - val_mean_absolute_error: 7.5943\n",
      "Epoch 772/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 1.7156 - mean_absolute_error: 1.7156 - val_loss: 7.3955 - val_mean_absolute_error: 7.3955\n",
      "Epoch 773/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.6662 - mean_absolute_error: 2.6662 - val_loss: 7.7098 - val_mean_absolute_error: 7.7098\n",
      "Epoch 774/800\n",
      "1164/1164 [==============================] - 0s 134us/sample - loss: 4.9972 - mean_absolute_error: 4.9972 - val_loss: 7.6714 - val_mean_absolute_error: 7.6714\n",
      "Epoch 775/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.8493 - mean_absolute_error: 2.8493 - val_loss: 7.4883 - val_mean_absolute_error: 7.4883\n",
      "Epoch 776/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 2.0231 - mean_absolute_error: 2.0231 - val_loss: 7.3672 - val_mean_absolute_error: 7.3672\n",
      "Epoch 777/800\n",
      "1164/1164 [==============================] - 0s 135us/sample - loss: 2.0835 - mean_absolute_error: 2.0835 - val_loss: 7.5070 - val_mean_absolute_error: 7.5070\n",
      "Epoch 778/800\n",
      "1164/1164 [==============================] - 0s 139us/sample - loss: 1.7005 - mean_absolute_error: 1.7005 - val_loss: 7.4331 - val_mean_absolute_error: 7.4331\n",
      "Epoch 779/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 1.6609 - mean_absolute_error: 1.6609 - val_loss: 7.5232 - val_mean_absolute_error: 7.5232\n",
      "Epoch 780/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 1.6667 - mean_absolute_error: 1.6667 - val_loss: 7.6112 - val_mean_absolute_error: 7.6112\n",
      "Epoch 781/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.2958 - mean_absolute_error: 2.2958 - val_loss: 7.8447 - val_mean_absolute_error: 7.8447\n",
      "Epoch 782/800\n",
      "1164/1164 [==============================] - 0s 145us/sample - loss: 2.7738 - mean_absolute_error: 2.7738 - val_loss: 7.8595 - val_mean_absolute_error: 7.8595\n",
      "Epoch 783/800\n",
      "1164/1164 [==============================] - 0s 146us/sample - loss: 3.1006 - mean_absolute_error: 3.1006 - val_loss: 7.5122 - val_mean_absolute_error: 7.5122\n",
      "Epoch 784/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 2.4122 - mean_absolute_error: 2.4122 - val_loss: 7.5093 - val_mean_absolute_error: 7.5093\n",
      "Epoch 785/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.2961 - mean_absolute_error: 2.2961 - val_loss: 7.4855 - val_mean_absolute_error: 7.4855\n",
      "Epoch 786/800\n",
      "1164/1164 [==============================] - 0s 144us/sample - loss: 2.6785 - mean_absolute_error: 2.6785 - val_loss: 7.6095 - val_mean_absolute_error: 7.6095\n",
      "Epoch 787/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 2.1175 - mean_absolute_error: 2.1175 - val_loss: 7.7879 - val_mean_absolute_error: 7.7879\n",
      "Epoch 788/800\n",
      "1164/1164 [==============================] - 0s 152us/sample - loss: 2.6659 - mean_absolute_error: 2.6659 - val_loss: 7.6405 - val_mean_absolute_error: 7.6405\n",
      "Epoch 789/800\n",
      "1164/1164 [==============================] - 0s 140us/sample - loss: 3.1402 - mean_absolute_error: 3.1402 - val_loss: 7.2218 - val_mean_absolute_error: 7.2218\n",
      "Epoch 790/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.2201 - mean_absolute_error: 2.2201 - val_loss: 7.5735 - val_mean_absolute_error: 7.5735\n",
      "Epoch 791/800\n",
      "1164/1164 [==============================] - 0s 143us/sample - loss: 2.5291 - mean_absolute_error: 2.5291 - val_loss: 7.6711 - val_mean_absolute_error: 7.6711\n",
      "Epoch 792/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 1.7169 - mean_absolute_error: 1.7169 - val_loss: 7.4262 - val_mean_absolute_error: 7.4262\n",
      "Epoch 793/800\n",
      "1164/1164 [==============================] - 0s 157us/sample - loss: 2.0099 - mean_absolute_error: 2.0099 - val_loss: 7.4462 - val_mean_absolute_error: 7.4462\n",
      "Epoch 794/800\n",
      "1164/1164 [==============================] - 0s 147us/sample - loss: 2.2425 - mean_absolute_error: 2.2425 - val_loss: 7.4454 - val_mean_absolute_error: 7.4454\n",
      "Epoch 795/800\n",
      "1164/1164 [==============================] - 0s 148us/sample - loss: 1.7611 - mean_absolute_error: 1.7611 - val_loss: 7.7050 - val_mean_absolute_error: 7.7050\n",
      "Epoch 796/800\n",
      "1164/1164 [==============================] - 0s 163us/sample - loss: 3.3826 - mean_absolute_error: 3.3826 - val_loss: 8.1781 - val_mean_absolute_error: 8.1781\n",
      "Epoch 797/800\n",
      "1164/1164 [==============================] - 0s 141us/sample - loss: 2.3066 - mean_absolute_error: 2.3066 - val_loss: 7.5464 - val_mean_absolute_error: 7.5464\n",
      "Epoch 798/800\n",
      "1164/1164 [==============================] - 0s 142us/sample - loss: 2.7057 - mean_absolute_error: 2.7057 - val_loss: 7.3478 - val_mean_absolute_error: 7.3478\n",
      "Epoch 799/800\n",
      "1164/1164 [==============================] - 0s 149us/sample - loss: 1.9837 - mean_absolute_error: 1.9837 - val_loss: 7.7694 - val_mean_absolute_error: 7.7694\n",
      "Epoch 800/800\n",
      "1164/1164 [==============================] - 0s 138us/sample - loss: 2.5071 - mean_absolute_error: 2.5071 - val_loss: 7.4725 - val_mean_absolute_error: 7.4725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe8801f3940>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(train, target, epochs=800, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn = train_rnn.reshape(1,train_rnn.shape[0],train_rnn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456, 1209)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 1456, 1209)        11698284  \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 120)               638400    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 12,336,805\n",
      "Trainable params: 12,336,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RNN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "RNN_model.add(LSTM(1209, kernel_initializer='normal',return_sequences=True,input_shape= (1456,1209)))\n",
    "\n",
    "# The Hidden Layers :\n",
    "RNN_model.add(LSTM(120))\n",
    "\n",
    "\n",
    "\n",
    "# The Output Layer :\n",
    "RNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "RNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rnn = target.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(target_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456, 1209)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rnn = target_rnn.reshape(1,target_rnn.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456, 1)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 0 batches). You may need to use the repeat() function when building your dataset.\n",
      "\r"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    186\u001b[0m   \u001b[0;31m# End of an epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m   \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Empty training data.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Empty training data.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-dcb003edf7ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'd/%d ['\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "RNN_model.fit(train_rnn, target_rnn, epochs=100, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear  total_cases\n",
       "0   sj  2008          18            0\n",
       "1   sj  2008          19            0\n",
       "2   sj  2008          20            0\n",
       "3   sj  2008          21            0\n",
       "4   sj  2008          22            0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>city_iq</th>\n",
       "      <th>city_sj</th>\n",
       "      <th>week_start_date_1990-04-30</th>\n",
       "      <th>week_start_date_1990-05-07</th>\n",
       "      <th>week_start_date_1990-05-14</th>\n",
       "      <th>week_start_date_1990-05-21</th>\n",
       "      <th>week_start_date_1990-05-28</th>\n",
       "      <th>week_start_date_1990-06-04</th>\n",
       "      <th>...</th>\n",
       "      <th>week_start_date_2013-04-23</th>\n",
       "      <th>week_start_date_2013-04-30</th>\n",
       "      <th>week_start_date_2013-05-07</th>\n",
       "      <th>week_start_date_2013-05-14</th>\n",
       "      <th>week_start_date_2013-05-21</th>\n",
       "      <th>week_start_date_2013-05-28</th>\n",
       "      <th>week_start_date_2013-06-04</th>\n",
       "      <th>week_start_date_2013-06-11</th>\n",
       "      <th>week_start_date_2013-06-18</th>\n",
       "      <th>week_start_date_2013-06-25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2008</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2008</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2008</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2008</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2008</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  weekofyear  city_iq  city_sj  week_start_date_1990-04-30  \\\n",
       "1456  2008          18        0        1                           0   \n",
       "1457  2008          19        0        1                           0   \n",
       "1458  2008          20        0        1                           0   \n",
       "1459  2008          21        0        1                           0   \n",
       "1460  2008          22        0        1                           0   \n",
       "\n",
       "      week_start_date_1990-05-07  week_start_date_1990-05-14  \\\n",
       "1456                           0                           0   \n",
       "1457                           0                           0   \n",
       "1458                           0                           0   \n",
       "1459                           0                           0   \n",
       "1460                           0                           0   \n",
       "\n",
       "      week_start_date_1990-05-21  week_start_date_1990-05-28  \\\n",
       "1456                           0                           0   \n",
       "1457                           0                           0   \n",
       "1458                           0                           0   \n",
       "1459                           0                           0   \n",
       "1460                           0                           0   \n",
       "\n",
       "      week_start_date_1990-06-04  ...  week_start_date_2013-04-23  \\\n",
       "1456                           0  ...                           0   \n",
       "1457                           0  ...                           0   \n",
       "1458                           0  ...                           0   \n",
       "1459                           0  ...                           0   \n",
       "1460                           0  ...                           0   \n",
       "\n",
       "      week_start_date_2013-04-30  week_start_date_2013-05-07  \\\n",
       "1456                           0                           0   \n",
       "1457                           0                           0   \n",
       "1458                           0                           0   \n",
       "1459                           0                           0   \n",
       "1460                           0                           0   \n",
       "\n",
       "      week_start_date_2013-05-14  week_start_date_2013-05-21  \\\n",
       "1456                           0                           0   \n",
       "1457                           0                           0   \n",
       "1458                           0                           0   \n",
       "1459                           0                           0   \n",
       "1460                           0                           0   \n",
       "\n",
       "      week_start_date_2013-05-28  week_start_date_2013-06-04  \\\n",
       "1456                           0                           0   \n",
       "1457                           0                           0   \n",
       "1458                           0                           0   \n",
       "1459                           0                           0   \n",
       "1460                           0                           0   \n",
       "\n",
       "      week_start_date_2013-06-11  week_start_date_2013-06-18  \\\n",
       "1456                           0                           0   \n",
       "1457                           0                           0   \n",
       "1458                           0                           0   \n",
       "1459                           0                           0   \n",
       "1460                           0                           0   \n",
       "\n",
       "      week_start_date_2013-06-25  \n",
       "1456                           0  \n",
       "1457                           0  \n",
       "1458                           0  \n",
       "1459                           0  \n",
       "1460                           0  \n",
       "\n",
       "[5 rows x 1209 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cpy = pd.read_csv('dengue_features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_precip_amt_kg_per_m2</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>18</td>\n",
       "      <td>2008-04-29</td>\n",
       "      <td>-0.0189</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>0.102729</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>78.60</td>\n",
       "      <td>298.492857</td>\n",
       "      <td>...</td>\n",
       "      <td>25.37</td>\n",
       "      <td>78.781429</td>\n",
       "      <td>78.60</td>\n",
       "      <td>15.918571</td>\n",
       "      <td>3.128571</td>\n",
       "      <td>26.528571</td>\n",
       "      <td>7.057143</td>\n",
       "      <td>33.3</td>\n",
       "      <td>21.7</td>\n",
       "      <td>75.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>19</td>\n",
       "      <td>2008-05-06</td>\n",
       "      <td>-0.0180</td>\n",
       "      <td>-0.012400</td>\n",
       "      <td>0.082043</td>\n",
       "      <td>0.072314</td>\n",
       "      <td>12.56</td>\n",
       "      <td>298.475714</td>\n",
       "      <td>...</td>\n",
       "      <td>21.83</td>\n",
       "      <td>78.230000</td>\n",
       "      <td>12.56</td>\n",
       "      <td>15.791429</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>26.071429</td>\n",
       "      <td>5.557143</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>34.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>20</td>\n",
       "      <td>2008-05-13</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151083</td>\n",
       "      <td>0.091529</td>\n",
       "      <td>3.66</td>\n",
       "      <td>299.455714</td>\n",
       "      <td>...</td>\n",
       "      <td>4.12</td>\n",
       "      <td>78.270000</td>\n",
       "      <td>3.66</td>\n",
       "      <td>16.674286</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>27.928571</td>\n",
       "      <td>7.785714</td>\n",
       "      <td>32.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>21</td>\n",
       "      <td>2008-05-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019867</td>\n",
       "      <td>0.124329</td>\n",
       "      <td>0.125686</td>\n",
       "      <td>0.00</td>\n",
       "      <td>299.690000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>73.015714</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.775714</td>\n",
       "      <td>4.342857</td>\n",
       "      <td>28.057143</td>\n",
       "      <td>6.271429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sj</td>\n",
       "      <td>2008</td>\n",
       "      <td>22</td>\n",
       "      <td>2008-05-27</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.039833</td>\n",
       "      <td>0.062267</td>\n",
       "      <td>0.075914</td>\n",
       "      <td>0.76</td>\n",
       "      <td>299.780000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.36</td>\n",
       "      <td>74.084286</td>\n",
       "      <td>0.76</td>\n",
       "      <td>16.137143</td>\n",
       "      <td>3.542857</td>\n",
       "      <td>27.614286</td>\n",
       "      <td>7.085714</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>84.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear week_start_date  ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0   sj  2008          18      2008-04-29  -0.0189 -0.018900  0.102729   \n",
       "1   sj  2008          19      2008-05-06  -0.0180 -0.012400  0.082043   \n",
       "2   sj  2008          20      2008-05-13  -0.0015       NaN  0.151083   \n",
       "3   sj  2008          21      2008-05-20      NaN -0.019867  0.124329   \n",
       "4   sj  2008          22      2008-05-27   0.0568  0.039833  0.062267   \n",
       "\n",
       "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "0  0.091200                 78.60             298.492857  ...   \n",
       "1  0.072314                 12.56             298.475714  ...   \n",
       "2  0.091529                  3.66             299.455714  ...   \n",
       "3  0.125686                  0.00             299.690000  ...   \n",
       "4  0.075914                  0.76             299.780000  ...   \n",
       "\n",
       "   reanalysis_precip_amt_kg_per_m2  reanalysis_relative_humidity_percent  \\\n",
       "0                            25.37                             78.781429   \n",
       "1                            21.83                             78.230000   \n",
       "2                             4.12                             78.270000   \n",
       "3                             2.20                             73.015714   \n",
       "4                             4.36                             74.084286   \n",
       "\n",
       "   reanalysis_sat_precip_amt_mm  reanalysis_specific_humidity_g_per_kg  \\\n",
       "0                         78.60                              15.918571   \n",
       "1                         12.56                              15.791429   \n",
       "2                          3.66                              16.674286   \n",
       "3                          0.00                              15.775714   \n",
       "4                          0.76                              16.137143   \n",
       "\n",
       "   reanalysis_tdtr_k  station_avg_temp_c  station_diur_temp_rng_c  \\\n",
       "0           3.128571           26.528571                 7.057143   \n",
       "1           2.571429           26.071429                 5.557143   \n",
       "2           4.428571           27.928571                 7.785714   \n",
       "3           4.342857           28.057143                 6.271429   \n",
       "4           3.542857           27.614286                 7.085714   \n",
       "\n",
       "   station_max_temp_c  station_min_temp_c  station_precip_mm  \n",
       "0                33.3                21.7               75.2  \n",
       "1                30.0                22.2               34.3  \n",
       "2                32.8                22.8                3.0  \n",
       "3                33.3                24.4                0.3  \n",
       "4                33.3                23.3               84.1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cpy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 1209)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_12 (SimpleRNN)    (None, 286)               427856    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 8)                 2296      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 430,161\n",
      "Trainable params: 430,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=286, input_shape=(1456,1209), activation=\"tanh\"))\n",
    "model.add(Dense(8, activation=\"relu\")) \n",
    "model.add(Dense(1,activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456, 1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = target.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456,)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = targ.reshape(1,targ.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456, 1)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1456, 1209)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "trai = train.values.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 1209)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (1, 1456, 1) was passed for an output of shape (None, 1) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-37f82f337610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    808\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    809\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (1, 1456, 1) was passed for an output of shape (None, 1) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "model.fit(train_rnn, targ, epochs=500, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A submission file has been made\n"
     ]
    }
   ],
   "source": [
    "def make_submission(prediction, sub_name):\n",
    "  my_submission = pd.DataFrame({'city':pd.read_csv('dengue_features_test.csv').city,\n",
    "                                'year':pd.read_csv('dengue_features_test.csv').year,\n",
    "                                'weekofyear':pd.read_csv('dengue_features_test.csv').weekofyear,\n",
    "                                'total_cases':prediction})\n",
    "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
    "  print('A submission file has been made')\n",
    "\n",
    "predictions = NN_model.predict(test).astype('int64')\n",
    "make_submission(predictions[:,0],'dnn2_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
